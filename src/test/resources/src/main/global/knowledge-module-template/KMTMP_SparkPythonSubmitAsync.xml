<?xml version="1.0" encoding="ISO-8859-1"?>
<SunopsisExport>
<Admin RepositoryVersion="05.02.02.09" OdiVersion="12.2.1" IsLegacyIdCompatible="false" IsExportWithParents="true" />
<Encryption algorithm="AES" keyLength="128" exportKeyHash="wTfCFGFR94y1kb+rkSaIY8xA1yCNZLeKu9RzRyW6VRY=" keyVect="7C0I1OGgHMBsapbyPEhbbA==" exportKeySalt="879cf7c1-74ba-45ee-be1c-d26c0b234ee9" containsCipherText="false"/>
<SmartExportList materializeShortcut="false" >
   <Include><![CDATA[SnpKmTemplate#153]]></Include>
</SmartExportList>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplate">
		<Field name="FirstDate" type="java.sql.Timestamp"><![CDATA[2020-04-17 19:45:20.0]]></Field>
	<Field name="FirstUser" type="java.lang.String"><![CDATA[SUNOPSIS_INSTALL]]></Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[fa46e0aa-2d00-11e6-93fc-00163e1ffd72]]></Field>
	<Field name="IndChange" type="java.lang.String"><![CDATA[I]]></Field>
	<Field name="IndIsSeeded" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IndLangTrans" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IndReplNl" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IntVersion" type="com.sunopsis.sql.DbInt"><![CDATA[3]]></Field>
	<Field name="IKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="LangName" type="java.lang.String"><![CDATA[PYTHON]]></Field>
	<Field name="LastDate" type="java.sql.Timestamp"><![CDATA[2020-04-17 21:01:10.0]]></Field>
	<Field name="LastUser" type="java.lang.String"><![CDATA[SUPERVISOR]]></Field>
	<Field name="Name" type="java.lang.String"><![CDATA[SparkPythonSubmitAsync]]></Field>
	<Field name="TechnoName" type="java.lang.String"><![CDATA[SPARK_PYTHON]]></Field>
	<Field name="TemplateText" type="java.lang.String"><![CDATA[
// Spark Python script generated by ODI
// ***Asynchronous*** Execution of command line:
// $[sparkPyExecFile]
//


// uncomment_for_java  next two lines
//public class YarnAsync {
//	private SnpReference odiRef;
	
	final String SPARK_SUBMIT_SHUTDOWN_POLLING_INTERVAL = "spark-submit-shutdown-polling-interval";
	final String SPARK_SUBMIT_SHUTDOWN_POLLING_RETRIES = "spark-submit-shutdown-polling-retries";
	final String SPARK_WEBUI_STARTUP_POLLING_INTERVAL = "spark-webui-startup-polling-interval";
	final String SPARK_WEBUI_POLLING_TOOL_RETRIES = "spark-webui-polling-tool-retries";
	final String SPARK_WEBUI_POLLING_TOOL_TIMEOUT = "spark-webui-polling-tool-timeout";
	final String SPARK_WEBUI_POLLING_TOOL_INTERVAL = "spark-webui-polling-tool-interval";
	final String SPARK_WEBUI_POLLING_REST_HTTP_CONNECTION_TIMEOUT = "spark-webui-polling-rest-http-connection-timeout";
	final String SPARK_WEBUI_POLLING_REST_HTTP_READ_TIMEOUT = "spark-webui-polling-rest-http-read-timeout";
	final String SPARK_WEBUI_REST_TIMEOUT = "spark-webui-rest-timeout";
	final String SPARK_WEBUI_STARTUP_POLLING_RETRIES = "spark-webui-startup-polling-retries";
	final String SPARK_WEBUI_STARTUP_POLLING_TOOL_TIMEOUT = "spark-webui-startup-polling-tool-timeout";
	final String SPARK_WEBUI_STARTUP_POLLING_HTTP_READ_TIMEOUT = "spark-webui-startup-polling-http-read-timeout";
	final String SPARK_WEBUI_STARTUP_POLLING_HTTP_CONNECTION_TIMEOUT = "spark-webui-startup-polling-http-connection-timeout";
	final String SPARK_WEBUI_STARTUP_POLLING_TOOL_INTERVAL = "spark-webui-startup-polling-tool-interval";

	final String SPARK_HISTORY_SERVER_URL = "spark-history-server-url";

	String SUCCEEDED = "SUCCEEDED";
	String KILLED = "KILLED";
	String FAILED = "FAILED";
	String SHUTTING_DOWN = "RemotingTerminator: Shutting down remote daemon";
	String SHUTDOWN_HOOK = "util.ShutdownHookManager: Shutdown hook called";
	List<String> completionMarkers = Arrays.asList(array(SUCCEEDED,
			KILLED, FAILED, SHUTTING_DOWN, SHUTDOWN_HOOK));

	// uncomment_for_java
//	public void main_method() throws Exception {

		SessionRestartType savedRestartType = null;
		String osCmd = "$[sparkPyExecFile]";
		/*
		 * SparkTestCode1.txt @TEST_CODE@ SparkTestCode1.txt
		 */
		info("Spark KM: Starting " + odiRef.getStep("STEP_NAME"));
		// if odiRef is null, it's because the definition was not
		// removed for groovy execution (around line 68)

		SparkJobInfo jobInfo = new SparkJobInfo();
		SparkStreamProcessor stdOutProcessor = new SparkStdOutStreamProcessor();
		SparkStreamProcessor stdErrProcessor = new SparkStdErrStreamProcessor();

		String stdOutDirective = getDataServerProperty(odiRef,
				"CAPTURE_OUT_STREAM", "ON_ERROR,ALL");
		String stdErrDirective = getDataServerProperty(odiRef,
				"CAPTURE_ERR_STREAM", "ON_ERROR,ALL");
		stdErrProcessor.init(stdOutProcessor, stdErrProcessor, odiRef, jobInfo,
				stdErrDirective);
		stdOutProcessor.init(stdOutProcessor, stdErrProcessor, odiRef, jobInfo,
				stdOutDirective);

		boolean isRestart = false;

		if ("$[odiExecutionMode]".equals("ASYNC")
				&& !"$[sparkEventLogEnabled]".equals("true")) {
			throw new OdiKMException(
					null,
					"ODIKM-SPARK-ASYNC-10000",
					"ODIKM-SPARK-ASYNC-10000: EKM option or DataServer property spark.eventLog.enabled must be set to true for asynchronous execution.",
					null);
		}

		// Determine whether this Spark application is submitted in yarn-cluster
		// mode

		String sparkMaster = odiRef.getDataServerInfo("DSERV_NAME",
				"$[logicalSchemaName]");
		String deployMode = odiRef.getDataServerProperty("deploy-mode", "SRC");
		boolean yarnClusterMode = false;
		if (sparkMaster.startsWith("yarn")
				&& (sparkMaster.endsWith("cluster") || deployMode
						.equals("cluster"))) {
			yarnClusterMode = true;
		}

		boolean yarnClientMode = false;
		if (sparkMaster.startsWith("yarn")
				&& (sparkMaster.endsWith("client") || deployMode
						.equals("client"))) {
			yarnClientMode = true;
		}
		boolean enableUnsupportedSparkModes = Boolean.valueOf(
				getDataServerProperty(odiRef,
						"odi.spark.enableUnsupportedSparkModes", "false"));

		if (!enableUnsupportedSparkModes && !yarnClientMode && !yarnClusterMode) {
			throw new OdiKMException(
					"ODIKM-SPARK-ASYNC-20011: spark-submit: Spark Master URL is set to "
							+ sparkMaster
							+ ". Only yarn-cluster and  yarn-client mode are supported. Please change spark Master URL on Spark DataServer or check documentation for more details. In case you intentionally want to use an unsupported Spark Deployment mode like Spark local mode, please consider adding property odi.spark.enableUnsupportedSparkModes=true to the Spark DataServer.");
		}

		// CHECKPOINT
		// Find out whether this is the first start of the Spark script
		String stepNo = odiRef.getStep("NNO");
		String keyAppId = "SparkAppId" + "." + stepNo;
		String keyAppName = "SparkAppName" + "." + stepNo;
		String keyLiveBaseUrl = "SparkLiveUrl" + "." + stepNo;
		String keyHistoryBaseUrl = "SparkHistoryUrl" + "." + stepNo;

		OdiInstance odiInstance = odiRef.getOdiInstance();
		HashMap<String, String> p = null;
		try {
			p = getInvocationParams(odiInstance,
					odiRef.getSession("SESS_NO"));
		} catch (Exception e) {
			e.printStackTrace();
		}

		String liveUrl = null;
		String liveStopUrl = null;
		
		try { // finally setExternalUrl()
			
		// If there is a liveBaseUrl stored for this Session task,
		// then this is a restart
		if (p.get(keyLiveBaseUrl) != null && p.get(keyLiveBaseUrl).length() > 0) {

			jobInfo.setAppId(p.get(keyAppId));
			jobInfo.setAppName(p.get(keyAppName));
			jobInfo.setLiveUrlBase(p.get(keyLiveBaseUrl));
			jobInfo.setHistoryServerUrl(p.get(keyHistoryBaseUrl));
			info(
					"Spark KM: Found Spark WebUI/REST API URL stored in ODI session: appId:",
					jobInfo.getAppId(), " appName:", jobInfo.getAppName(),
					" liveBaseUrl:" + jobInfo.getLiveUrlBase(),
					" historyBaseUrl:" + jobInfo.getHistoryServerUrl());

			isRestart = true;
		} else {
			// if not, then this is the very first start. So we need to submit
			// the Spark job:
			OSCommand tool = new OSCommand();
			Vector<String[]> params = new Vector<String[]>();
			params.add(array("-COMMAND", osCmd));
			params.add(array("-OUT_FILE", "$[sparkPyOutFile]"));
			params.add(array("-ERR_FILE", "$[sparkPyErrFile]"));
			params.add(array("-WORKING_DIR", "$[sparkPyWorkDir]"));
			params.add(array("-FILE_APPEND", "false"));
			params.add(array("-SYNCHRONOUS", "false"));
			params.add(array("-CAPTURE_OUT_STREAM", 
					getDataServerProperty(odiRef, "CAPTURE_OUT_STREAM",
							"ON_ERROR,ALL")));
			params.add(array("-CAPTURE_ERR_STREAM", 
					getDataServerProperty(odiRef, "CAPTURE_ERR_STREAM",
							"ON_ERROR,ALL")));
			tool.setParameters(params);
			Map<String, String> env = odiRef.getDataServerProperties("SRC");
			tool.putEnvironmentVariables(env);
			tool.setOutputStreamProcessor(stdOutProcessor);
			tool.setErrorStreamProcessor(stdErrProcessor);
			// Launch spark-submit command
			info("Spark KM: Submitting os command: ",
					tool.getCommandLine());
			char[] msg = ("Arg Environment: "
					+ String.valueOf(tool.getArgEnvironment()) + "\n")
					.toCharArray();
			stdOutProcessor.getRingBuffer().add(msg, 0, msg.length);

			tool.execute();
			// Monitor stdout/stderr to retrieve Spark Live WebUI URL (and other
			// information)
			int maxRetries = getDataServerProperty(odiRef,
					SPARK_WEBUI_STARTUP_POLLING_RETRIES, 12);
			int pollingInterval = getDataServerProperty(odiRef,
					SPARK_WEBUI_STARTUP_POLLING_INTERVAL, 5000);
			stdOutProcessor.setPersistInterval(getDataServerProperty(odiRef,
					SPARK_WEBUI_STARTUP_POLLING_PERSIST_INTERVAL, 0));
			stdErrProcessor.setPersistInterval(getDataServerProperty(odiRef,
					SPARK_WEBUI_STARTUP_POLLING_PERSIST_INTERVAL, 0));
			int remainingRetries = maxRetries;

			jobInfo.setHistoryServerUrl(odiRef.getDataServerProperty(
					SPARK_HISTORY_SERVER_URL, "SRC"));
			while (!minParamsAvailable(jobInfo.getSparkWebUIUrl(),
					jobInfo.getAppId(), jobInfo.getAppName(),
					jobInfo.getHistoryServerUrl())
					&& remainingRetries-- > 0) {
				finest("Spark KM: Checking stdout/stderr for Spark WebUI/REST API URL...");
				info(
						"Spark KM: Checking stdout/stderr for Spark WebUI/REST API URL: appId:",
						jobInfo.getAppId(), " appName:", jobInfo.getAppName(),
						" liveBaseUrl:" + jobInfo.getLiveUrlBase(),
						" historyBaseUrl:" + jobInfo.getHistoryServerUrl());
				if (stdErrProcessor.hasFinished()) {
					Integer rc = jobInfo.getReturnCode();
					if (rc == null) {
						throw new OdiKMException(
								null,
								"ODIKM-SPARK-ASYNC-20007",
								"ODIKM-SPARK-ASYNC-20007: Internal error: os process finished with no os return code reported!",
								null);
					}
					if (rc == 0) {
						finest("Spark KM: os process has finished with return code 0");
						throw new OdiKMException(
								null,
								"ODIKM-SPARK-ASYNC-20008",
								"ODIKM-SPARK-ASYNC-20008: os process finished before reporting Spark parameters for monitoring.",
								null);
					} else {
						throw new OdiKMException(
								null,
								"ODIKM-SPARK-ASYNC-20009",
								"ODIKM-SPARK-ASYNC-20009: os process finished with return code {0} Exception: {1}",
								array(
										jobInfo.getReturnCode(),
										stdOutProcessor.getExceptionCaught() != null ? stdOutProcessor
												.getExceptionCaught()
												: stdErrProcessor
														.getExceptionCaught()));
					}
				}
				stdOutProcessor.persistCurrentTaskInfoWhenNeeded();
				try {
					stdErrProcessor.setParentInterruptible(true);
					Thread.sleep(pollingInterval);
				} catch (InterruptedException e) {
					// We may get interupted by StdErrProcessor
					info("wait for getSparkWebUIUrl() was interrupted");
				} finally {
					stdErrProcessor.setParentInterruptible(false);
				}
			}

			if (jobInfo.getHistoryServerUrl() == null
					|| jobInfo.getHistoryServerUrl().length() == 0)
				throw new OdiKMException(
						null,
						"ODIKM-SPARK-ASYNC-10001",
						"ODIKM-SPARK-ASYNC-10001: DataServer property spark-history-server-url is missing and is mandatory for asynchronous execution.",
						null);
			// String yarnUrl = jobInfo.getYarnTrackingUrl();
			// String yarnAppId = jobInfo.getYarnAppId();
			fine(
					"Spark KM: Retrieved following values from stdout/stderr: : appId:",
					jobInfo.getAppId(), " appName:", jobInfo.getAppName(),
					" liveBaseUrl:" + jobInfo.getLiveUrlBase(),
					" historyBaseUrl:" + jobInfo.getHistoryServerUrl());

			// Did we receive all required Spark REST API connection details?
			if (!minParamsAvailable(/* yarnUrl, yarnAppId, */
			jobInfo.getLiveUrlBase(), jobInfo.getAppId(), jobInfo.getAppName(),
					jobInfo.getHistoryServerUrl())) {
				throw new OdiKMException(
						null,
						"ODIKM-SPARK-ASYNC-20000",
						"ODIKM-SPARK-ASYNC-20000: Could not detect Spark Live REST API connection details after {0} retries. Values detected:  appId:\"{1}\" appName:\"{2}\" liveBaseUrl:\"{3}\" historyBaseUrl:\"{4}\" yarnUrl:\"{5}\" yarnAppId:\"{6}\"",
						array(maxRetries, jobInfo.getAppId(),
								jobInfo.getAppName(), jobInfo.getLiveUrlBase(),
								jobInfo.getHistoryServerUrl()));
			}

			if (yarnClusterMode || yarnClientMode) {

				// CHECKPOINT - (for yarn-cluster mode)
				// At this point we have the yarn tracking url and the yarn
				// application id
				// Next we need to wait for the Yarn job to complete and check
				// its job status
				// After job completion the updated tracking URL reported by the
				// yarn-client should be
				// stored into the ODI Operator session task URL.
				// ...

			} else {

				// CHECKPOINT - (not for yarn-cluster mode)
				// At this point we have connection details for the Spark Live
				// REST API and Spark History Server REST API
				// Next we need to wait for the Spark Live REST API to come up
			}

			// CHECKPOINT
			// At this point the Spark History server (or Yarn RM in case of
			// yarn-cluster mode)
			// has reported job completion
			// The OS process should finish now any second.

			// CHECKPOINT
			// At this point the OS process has completed successfully
			// Save task summary message (may be cut it down)

			// Save values in repository
			HashMap<String, String> map = new HashMap<String, String>();
			map.put(keyAppId, jobInfo.getAppId());
			map.put(keyAppName, jobInfo.getAppName());
			map.put(keyLiveBaseUrl, jobInfo.getLiveUrlBase());
			map.put(keyHistoryBaseUrl, jobInfo.getHistoryServerUrl());

			try {
				saveInvocationParams(odiInstance,
						odiRef.getSession("SESS_NO"), map);
				savedRestartType = updateSessionRestartType(odiInstance,
						odiRef.getSession("SESS_NO"),
						SessionRestartType.RUN_FROM_TASK);
			} catch (Exception e) {
				e.printStackTrace();
			}
			// Set the Web UI Url in ODI Operator
			odiRef.setExternalUrl(jobInfo.getSparkWebUIUrl());
			odiRef.persistCurrentTaskInfo();
		} // not a restart

		// CHECKPOINT
		// At this point we have connection details for the Spark Live REST API
		// and Spark History Server REST API
		// Next we need to wait for the Spark Live REST API to come up
		// Build url for checking live application status

		// If the keyword is set, this will disconnect the repository,
		// the session will error-out, but be left
		// in "running" because it cannot update the status in the repostiroy
		odiRef.doInstrumentation(SessionInstrumentType.INSTR_BEFORE_SESSION_COMPLETE);

		if (sparkMaster.startsWith("local")) {
			liveUrl = concat(jobInfo.getLiveUrlBase() , "api/v1/applications/"
					, jobInfo.getAppId());
			liveStopUrl = concat(jobInfo.getLiveUrlBase() ,"api/v1/applications/"
					, jobInfo.getAppId() , "/state");
		} else {
			liveUrl = concat(jobInfo.getLiveUrlBase() , "ws/v1/cluster/apps/"
					, jobInfo.getAppId());
			liveStopUrl = concat(jobInfo.getLiveUrlBase() , "ws/v1/cluster/apps/"
					, jobInfo.getAppId(), "/state");
		}

		// Integer webuiRestTimeout = getDataServerPropertyWithDefault(
		// SPARK_WEBUI_REST_TIMEOUT, 0); // not referenced?
		Integer maxRetries = getDataServerProperty(odiRef,
				SPARK_WEBUI_STARTUP_POLLING_RETRIES, 1);
		// Wait max of 20 seconds for WebUI to come up
		Integer timeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_STARTUP_POLLING_TOOL_TIMEOUT, 20000);
		Integer pollingInterval = getDataServerProperty(odiRef,
				SPARK_WEBUI_STARTUP_POLLING_TOOL_INTERVAL, 2000);
		Integer httpConnectionTimeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_STARTUP_POLLING_HTTP_CONNECTION_TIMEOUT, 10000);
		Integer httpReadTimeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_STARTUP_POLLING_HTTP_READ_TIMEOUT, 10000);
		stdOutProcessor.setPersistInterval(getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_PERSIST_INTERVAL, 0));
		stdErrProcessor.setPersistInterval(getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_PERSIST_INTERVAL, 0));
		fine("Spark KM: Waiting for first Application Status response using JSON REST call on $liveUrl with REST timeout ???? ...");
		WaitForSparkJob waitCommand = buildWaitForSparkJobCommand(liveUrl,
				liveStopUrl, timeout, pollingInterval, httpConnectionTimeout,
				httpReadTimeout, maxRetries);
		boolean isWebuiUp = false;
		// Loop until first successful response received from WebUI...
		info("waiting on :", waitCommand.getCommandLine());
		try {
			// Start polling for Spark Application status
			odiRef.configureTool(waitCommand);
			stdErrProcessor.setParentInterruptible(true);
			waitCommand.execIntegratedFunction();
			Object response = waitCommand.getResult();
			info("Spark KM: REST call returned: ", response);
			isWebuiUp = (response != null);
		} catch (InterruptedException e) {
			// We may get interupted by StdErrProcessor (very early
			// termination, should not happen) or by Stop Immediate
			severe("Spark KM: ODI session has been stopped via stop immediate.");
			throw new OdiKMException(
					e,
					"ODIKM-SPARK-ASYNC-20010",
					"ODIKM-SPARK-ASYNC-20010: Spark KM: ODI session has been stopped via stop graceful or stop immediate.",
					null);
		} catch (Exception e) {
			// We have to interpret any exception, as
			// "The Spark Live REST API is down."
			info("Spark KM: Exception while waiting for first WebUI response while connecting to Live JSON REST API (This usually means that the Spark application has finished.): ${e.getMessage()}");
			throw new OdiKMException(e, "ODIKM-SPARK-ASYNC-20012",
					"ODIKM-SPARK-ASYNC-20012: Spark KM: unexpected exception",
					null);
		} finally {
			stdErrProcessor.setParentInterruptible(false);
		}
		if (!isWebuiUp) {
			throw new OdiKMException(
					new Exception("webui not up"),
					"ODIKM-SPARK-ASYNC-20001",
					"ODIKM-SPARK-ASYNC-20001: No first response from Spark when polling Spark Application status on Spark History Server JSON REST API on \"{0}\"",
					array(liveUrl));
		}
		info("Spark KM: first WebUI response received. ");
		/*
		 * CHECKPOINT At this point we have been able to connect to the Spark
		 * Live REST API. Next we need to wait until the the API stops
		 * responding (end of Spark application). For endless streaming jobs
		 * this may take forever. The following section can be interrupted by
		 * "Stop graceful"
		 */
		maxRetries = getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_TOOL_RETRIES, 1); // No retries. If no
		// more response =>
		// Spark WebUI has been
		// shutdown
		timeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_TOOL_TIMEOUT, 0); // Wait for unlimited
		// amount of time
		pollingInterval = getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_TOOL_INTERVAL, 5000);
		httpConnectionTimeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_REST_HTTP_CONNECTION_TIMEOUT, 10000);
		httpReadTimeout = getDataServerProperty(odiRef,
				SPARK_WEBUI_POLLING_REST_HTTP_READ_TIMEOUT, 10000);
		// waitFor_Name = "attempts.completed"
		String waitFor_Name = "app.state";
		String waitFor_Value = "FINISHED,KILLED";
		String waitFor_Cond = "ONE_OF";
		waitCommand = buildWaitForSparkJobCommand(liveUrl, liveStopUrl,
				timeout, pollingInterval, httpConnectionTimeout,
				httpReadTimeout, maxRetries, waitFor_Name, waitFor_Value,
				waitFor_Cond);
		odiRef.configureTool(waitCommand);
		info("Spark KM: Polling using ", waitCommand.getCommandLine());

		try {
			stdErrProcessor.setParentInterruptible(true);
			waitCommand.execIntegratedFunction();
		} catch (InterruptedException e) {
			severe("Spark KM: ODI session has been stopped via stop immediate.");
			throw new OdiKMException(
					e,
					"ODIKM-SPARK-ASYNC-20010",
					"ODIKM-SPARK-ASYNC-20010: Spark KM: ODI session has been stopped via stop immediate.",
					null);
		} finally {
			stdErrProcessor.setParentInterruptible(false);
		}
		Object response = waitCommand.getResult();
		finest("Spark KM: REST call returned: ", response);
		// CHECKPOINT
		// At this point the Spark application has stopped responding (which
		// usually means that it has completed)
		// Next we need to verify completion using the Spark History Server,
		// which by now should have picked up
		// the logs from the Spark application.
		// For now the History server does not give a timely and reliable
		// information about the job completion status.
		// Therefore this validation is skipped.
		/*
		 * // Start of Disabled History Server validation // Build url for
		 * 
		 * ... End of Disabled History Server validation
		 */
		// CHECKPOINT
		// At this point the Spark History server has reported job completion.
		// The OS process should finish now any second.

			if (isRestart) {
				// in case of a restart there is no OS process info. So we
				// cannot
				// check for OS process completion
				// and assume successful job completion
				// This means that after Agent failure/restart there is no
				// validation of Success-Marker nor of OS
				// return code
			} else {
				// Let"s wait for the OS process to complete
				maxRetries = getDataServerProperty(odiRef,
						SPARK_SUBMIT_SHUTDOWN_POLLING_RETRIES, 60);
				pollingInterval = getDataServerProperty(odiRef,
						SPARK_SUBMIT_SHUTDOWN_POLLING_INTERVAL, 5000);
				stdOutProcessor.setPersistInterval(getDataServerProperty(odiRef,
						SPARK_SUBMIT_SHUTDOWN_POLLING_PERSIST_INTERVAL, 0));
				stdErrProcessor.setPersistInterval(getDataServerProperty(odiRef,
						SPARK_SUBMIT_SHUTDOWN_POLLING_PERSIST_INTERVAL, 0));
				int remainingRetries = maxRetries;
				while ((!stdErrProcessor.hasFinished() || !stdOutProcessor
						.hasFinished()) && remainingRetries-- > 0) {
					fine(
							"Spark KM: Waiting for OS process to finish... : ",
							remainingRetries);
					stdOutProcessor.persistCurrentTaskInfoWhenNeeded();
					try {
						stdErrProcessor.setParentInterruptible(true);
						Thread.sleep(pollingInterval);
					} catch (InterruptedException e) {
						// We get interupted by StdErrProcessor
						// when os process finishes or
						// when success marker has been received
					} finally {
						stdErrProcessor.setParentInterruptible(false);
					}
				}

				updateSessionRestartType(odiInstance,
						odiRef.getSession("SESS_NO"), savedRestartType);

				odiRef.setExternalUrl(jobInfo.getSparkWebUIUrl());

				if (!stdErrProcessor.hasFinished()) {
					throw new OdiKMException(
							null,
							"ODIKM-SPARK-ASYNC-20005",
							"ODIKM-SPARK-ASYNC-20005: Spark Application has finished, but OS process spark-submit did not finish after {0} retries with polling interval {1} milliseconds. "
									+ Thread.currentThread(), array(
									maxRetries, pollingInterval));
				}
				if (!stdOutProcessor.hasFinished()) {
					throw new OdiKMException(
							null,
							"ODIKM-SPARK-ASYNC-20008",
							"ODIKM-SPARK-ASYNC-20008: Spark Application has finished, but OS process spark-submit did not finish after {0} retries with polling interval {1} milliseconds. "
									+ Thread.currentThread(), array(
									maxRetries, pollingInterval));
				}
				// The last thing to do is to verify that the Python script has
				// finished *successfully*
				if (jobInfo.getReturnCode() != 0) {
					throw new OdiKMException(
							null,
							"ODIKM-SPARK-ASYNC-20009",
							"ODIKM-SPARK-ASYNC-20009: os process finished with return code {0} Exception: {1}",
							array(
									jobInfo.getReturnCode(),
									stdOutProcessor.getExceptionCaught() != null ? stdOutProcessor
											.getExceptionCaught()
											: stdErrProcessor
													.getExceptionCaught()));

				}

				if (jobInfo.getCompletionMarkerReceived() == null) {
					throw new OdiKMException(
							null,
							"ODIKM-SPARK-ASYNC-20006",
							"ODIKM-SPARK-ASYNC-20006: Spark Application has finished, but has not reported successful completion. "
									+ Thread.currentThread(), null);
				} else if (!completionMarkers.contains(jobInfo
						.getCompletionMarkerReceived())) { // ok for local (also
					// client?)) {
					throw new OdiKMException(
							null,
							"ODIKM-SPARK-ASYNC-20007",
							"ODIKM-SPARK-ASYNC-20007: Spark Application has finished, but has reported completion code: "
									+ jobInfo.getCompletionMarkerReceived()
									+ Thread.currentThread(), null);
				}
			}
                } finally {
                        try {
                                if (jobInfo.getHistoryServerUrl() != null && jobInfo.getHistoryServerUrl().length() > 0 && jobInfo.getAppId() != null) {
                                        odiRef.setExternalUrl(jobInfo.getHistoryServerUrl()+"/proxy/"+jobInfo.getAppId());
                                        odiRef.persistCurrentTaskInfo();
                                }
                        } catch (Exception e) {
                                // Ignore any exception during persisting
                                // We do not want to stop the Spark execution because of
                                // some overlapping repo task update.
                        }
		}

		// CHECKPOINT
		// At this point the OS process has completed successfully
		// Save task summary message (may be cut it down)
	
		// uncomment_for_java next two lines
//	}
//}
    ]]></Field>
	<Field name="TemplateVersion" type="java.lang.String">null</Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[537af668-69e0-4350-b4fc-776edad8796d]]></Field>
	<Field name="IKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[94]]></Field>
	<Field name="IKmTemplateRef" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[8b1dd34d-173b-4e83-9ec3-8a044231a6eb]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[142]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[d2553e0e-3328-4c71-b1cc-5310a73f8ae9]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[146]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[8f0004ad-36a5-439f-8996-91290e03780c]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[150]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[a4209277-b80e-4876-86a5-b9b64d99afc3]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[152]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[22ac2d6b-ec78-403b-98d8-ced5e6031085]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[141]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[c630db92-df85-4d3e-891a-1996f6c314a6]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[139]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[75d35ef9-7e1b-449e-9b35-edcc19d9c798]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[88efacbd-aac2-4029-9b4a-cb9ea15abf6e]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[154]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[5236705a-54a5-4e9a-838f-ed19216d9bdd]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[155]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[6b5b896c-2ac8-4a82-92b5-89bf20357f30]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[169]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[80b15be6-8dc9-4073-86d7-b01796cc2114]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[153]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[168]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.94]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46e05a-2d00-11e6-93fa-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmit]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[17]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.153]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46e0aa-2d00-11e6-93fc-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmitAsync]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[22]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.142]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[33920E0F49656934E053E603C40A3C0A]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKMSparkHDFS]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[12]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.146]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[2B7D95B0B8A32C45E053E603C40A5DCB]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to HDFS]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[17]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.150]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[c2f113d5-e22b-4237-b25c-545872875536]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to Hive]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[17]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.152]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[dabbcab5-caeb-423c-b6f2-bfb106747464]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to Kafka]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[18]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.141]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[1986feab-eca2-4f5a-96a8-ecf827ac7cda]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to File]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[17]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.139]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[a7c7dc44-16fc-4b90-9c67-fc41cb76446d]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKMSparkFile]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[12]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.138]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[80aa8be2-d68d-40fe-bc1e-0b0b3ee0105e]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKMSpark]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[8]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.154]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[1FD1D82B9F005A53E053CCC0E40AC733]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to SQL]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[16]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.155]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[28C4768839C04BAEE053CCC0E40AAAF7]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKM Spark to Cassandra]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[22]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.169]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[310d86cf-27e0-48c0-ae73-82685449bcce]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[IKM Spark Table Function]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[24]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.168]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[2614CF93-4394-408B-82F5-A7F586F627AE]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[XKM Spark Table Function]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[24]]></Field>
</Object>
<Object class="com.sunopsis.dwg.DwgExportSummary">
		<Field name="ExpTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="InstObjNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="LinkDiagNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MorigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MtxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OrigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OtherObjectsNb" type="com.sunopsis.sql.DbInt"><![CDATA[13]]></Field>
	<Field name="PlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="StepNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="TxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="UeOrigNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="UeUsedNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="VarPlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="ScenTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OdiVersion" type="java.lang.String"><![CDATA[12.2.1]]></Field>
	<Field name="OriginRepositoryID" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="RepositoryVersion" type="java.lang.String"><![CDATA[05.02.02.09]]></Field>
</Object>
</SunopsisExport>
