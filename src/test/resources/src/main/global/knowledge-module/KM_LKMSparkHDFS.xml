<?xml version="1.0" encoding="ISO-8859-1"?>
<SunopsisExport>
<Admin RepositoryVersion="05.02.02.08" IsLegacyIdCompatible="false" />
<Encryption algorithm="AES" keyLength="128" exportKeyHash="Op8r3EBN0ZDJpOmvAOnAuo2/tQzS4p0arTPEzclKxYI=" keyVect="O8nYkGsKCbKNlcPxXQz7+A==" exportKeySalt="ab7374e7-715e-465d-a921-327ab60c0d9f" containsCipherText="false"/>
<Object class="com.sunopsis.dwg.dbobj.SnpTrt">
		<Field name="CleanupOnError" type="java.lang.String">null</Field>
	<Field name="CompType" type="java.lang.String"><![CDATA[AP]]></Field>
	<Field name="DelegateClass" type="java.lang.String"><![CDATA[LKMSparkHDFS]]></Field>
	<Field name="DelegateScript" type="java.lang.String"><![CDATA[

      import com.sunopsis.tools.core.SnpsStringTools;
	import oracle.odi.mapping.generation.AbstractSyntaxTree.ASTProducerMethod;
	import oracle.odi.domain.mapping.generator.KMGeneratorDelegateRegistry.GeneratorDelegateClass;
	import oracle.odi.domain.mapping.exception.*
        import oracle.odi.domain.model.OdiDataStore;
	@GeneratorDelegateClass(componentType="AP", descriptionKey="GeneratorDelegateClass.LKMSparkHDFS")
	public class LKMSparkHDFS extends LKMSpark {
		static { 
		    LKMSpark.storageClasses.put(LKMSparkStreamingTextLoader.LOADER_FUNCTION,LKMSparkStreamingTextLoader.class);
	            LKMSpark.storageClasses.put(LKMSparkJsonHDFSLoader.LOADER_FUNCTION,LKMSparkJsonHDFSLoader.class);
	            LKMSpark.storageClasses.put(LKMSparkParquetLoader.LOADER_FUNCTION,LKMSparkParquetLoader.class);
	            LKMSpark.storageClasses.put(LKMSparkNewHadoopFileLoader.LOADER_FUNCTION,LKMSparkNewHadoopFileLoader.class); 
	            LKMSpark.storageClasses.put(LKMSparkDelimitedLoader.LOADER_FUNCTION,LKMSparkDelimitedLoader.class);
	            LKMSpark.storageClasses.put(LKMSparkJsonHDFSStore.STORAGE_FUNCTION,LKMSparkJsonHDFSStore.class);
	            LKMSpark.storageClasses.put(LKMSparkAVROHDFSStore.STORAGE_FUNCTION,LKMSparkAVROHDFSStore.class);
	            LKMSpark.storageClasses.put(LKMSparkPARQUETHDFSStore.STORAGE_FUNCTION,LKMSparkPARQUETHDFSStore.class);
	            LKMSpark.storageClasses.put(LKMSparkDelimitedLoader.STORAGE_FUNCTION,LKMSparkDelimitedLoader.class);
	      }

		static class LKMSparkStreamingTextLoader extends LKMSpark.LKMSparkStorage { 
                    static final String LOADER_FUNCTION = "textFileStream"; 
                    @Override
                    String getLoaderFunction() { return LOADER_FUNCTION;} 
                    @Override
                    String getStorageFunctionParamString(List<LKMSpark.FunctionParam> funcParams, List<LKMSpark.FunctionOption> funcOptions, final boolean isLoader) throws AdapterException, GenerationException, MappingException {
                        return "protocol+strLoc";
                    }
                }

		static class LKMSparkJsonHDFSLoader extends LKMSpark.LKMSparkStorage { 
		    static final String LOADER_FUNCTION = "json"; 
		    @Override
		    String getLoaderFunction() {
		        return LOADER_FUNCTION;
		    } 
		} 
		
		static class LKMSparkParquetLoader extends LKMSpark.LKMSparkStorage { 
		    static final String LOADER_FUNCTION = "parquet"; 
		    @Override
		    String getLoaderFunction() {
		        return LOADER_FUNCTION;
		    } 
		}
		
		static class LKMSparkHadoopLoader extends LKMSpark.LKMSparkStorage { 

		    @Override
		    void addCommand(SparkCommand command) {
		       storageCommand = command;
		    }   
		    
		    @Override
		    String handleFunctionParam(LKMSpark.FunctionParam param,
		                             String value,
		                             boolean isLoader) throws AdapterException,
		                                          MappingException {
		        if (param.name.equals("Job Configuration") && (value == null || value == "None" || value == "")) {
		            if (storageCommand !=null && storageCommand instanceof SparkLoadCmd){
		            	String rowSep = ((SparkLoadCmd)storageCommand).getRowSep();
		            	if (rowSep != null && rowSep != ""){
		            			return "{\"textinputformat.record.delimiter\":\""+ rowSep + "\"}";
		            		}
		            }
		        } else if ((param.name.equals(LKMSpark.LKMSparkStorage.PARAM_KEY_CLASS) 
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_VALUE_CLASS)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_KEY_CONVERTER) 
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_VALUE_CONVERTER)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_OUTPUT_FORMAT)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_INPUT_FORMAT))
		        	  && value != "None" && value != null && value != "")		{
								value = "'" + value + "'";		        	
		        }      		        		        
		        return value;
		    }    
		}

		static class LKMSparkHadoopStorer extends LKMSpark.LKMSparkStorage { 

		    @Override
		    void addCommand(SparkCommand command) {
		       storageCommand = command;
		    }   
		    
		    @Override
		    String handleFunctionParam(LKMSpark.FunctionParam param,
		                             String value,
		                             boolean isLoader) throws AdapterException,
		                                          MappingException {
						if ((param.name.equals(LKMSpark.LKMSparkStorage.PARAM_KEY_CLASS) 
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_VALUE_CLASS)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_KEY_CONVERTER) 
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_VALUE_CONVERTER)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_OUTPUT_FORMAT)
		        		|| param.name.equals(LKMSpark.LKMSparkStorage.PARAM_INPUT_FORMAT))
		        	  && value != "None" && value != null && value != "")		{
								value = "'" + value + "'";		        	
		        }      		        		        
		        return value;
		    }    
		}

		static class LKMSparkNewHadoopFileLoader extends LKMSparkHadoopLoader { 
		    static final String LOADER_FUNCTION = "newAPIHadoopFile"; 
		    static List<LKMSpark.FunctionParam> FUNCT_ARG = new ArrayList<LKMSpark.FunctionParam>();
		    
		    static {
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_INPUT_FORMAT));
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_KEY_CLASS));
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_VALUE_CLASS));
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_KEY_CONVERTER));
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_VALUE_CONVERTER));
		        FUNCT_ARG.add(new LKMSpark.FunctionParam(LKMSpark.LKMSparkStorage.PARAM_CONF_NAME));		          
		    }
		    
		    @Override
		    String getLoaderFunction() {
		        return LOADER_FUNCTION;
		    } 
		    
		   	@Override
		    List<LKMSpark.FunctionParam> getLoaderFunctionParams() {
		        return FUNCT_ARG;
		    }
		}

		static class LKMSparkJsonHDFSStore extends LKMSpark.LKMSparkStorage { 
		    static final String STORAGE_FUNCTION = "json"; 
		    @Override
		    String getLoaderFunction() {
		        return STORAGE_FUNCTION;
		    }
		
		    @Override
		    String getStorerFunction() {
		  	  return STORAGE_FUNCTION;
		    } 
		}
		
		static class LKMSparkAVROHDFSStore extends LKMSpark.LKMSparkStorage { 
		    static final String STORAGE_FUNCTION = "com.databricks.spark.avro"; 
		    @Override
		    String getLoaderFunction() {
		        return STORAGE_FUNCTION;
		    }
		
		    @Override
		    String getStorerFunction() {
		  	  return STORAGE_FUNCTION;
		    } 
		}
		static class LKMSparkPARQUETHDFSStore extends LKMSpark.LKMSparkStorage { 
		    static final String STORAGE_FUNCTION = "parquet"; 
		    @Override
		    String getLoaderFunction() {
		        return STORAGE_FUNCTION;
		    }
		
		    @Override
		    String getStorerFunction() {
		  	  return STORAGE_FUNCTION;
		    } 
		}
		
		static class LKMSparkDelimitedLoader extends LKMSpark.LKMSparkStorage { 
		    static final String LOADER_FUNCTION = "csv"; 
		    static final String STORAGE_FUNCTION = "csv";
		    @Override
		    String getLoaderFunction() {
		        return LOADER_FUNCTION;
		    }
		    		
		    @Override
		    String getStorerFunction() {
		  	  return STORAGE_FUNCTION;
		    }  	    
		}
		
		String getDelimDataStoreStorageValue(MapPhysicalNode dsNode, MapPhysicalNode currentNode) throws AdapterException, MappingException {
System.out.println("======================================");
System.out.println("getDelimDataStoreStorageValue : 1");
System.out.println("======================================");

                  OdiDataStore dataStore = (OdiDataStore)dsNode.getLogicalComponent().getBoundObject();
                  StringBuilder strBuilder = new StringBuilder();
                  
                  String fieldSeparator = dataStore.getHadoopStorageDescriptor().getFieldSeparator();
                  String textDelimiter = dataStore.getHadoopStorageDescriptor().getTextDelimiter();
                  int headerLines = dataStore.getHadoopStorageDescriptor().getSkipHeadingLines();
                  def inferSourceSchema = LKMSpark.LKMSparkStorage.getOptionValue(currentNode, "inferSchema")
                  def dateFormat = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "dateFormat")
                  
                  strBuilder.append("options(");
                  
                  if (inferSourceSchema != null) {
                     boolean infer = inferSourceSchema.equalsIgnoreCase("true") ? true : false
			   if ( infer ) {
			    strBuilder.append("inferSchema='true', ")
			   }
			}
                  if (headerLines > 0 ) {
                      strBuilder.append("header='true'");
                  } else {
                      strBuilder.append("header='false'");
                  }
                  if(fieldSeparator != null || textDelimiter != null ){
			    strBuilder.append(", delimiter=");
                      if (textDelimiter != null) {
				strBuilder.append("'");
				strBuilder.append(textDelimiter);
				strBuilder.append("'");
                      } else {
				if(fieldSeparator.equalsIgnoreCase("0x09") || fieldSeparator.equalsIgnoreCase('\\u0009')){
				  strBuilder.append("'\\t'");
				} else {
				  strBuilder.append("'");
				  strBuilder.append(fieldSeparator);
				  strBuilder.append("'");
				}
                      }
                  }
                  strBuilder.append(', nullValue=""');
                  if (dateFormat != null && dateFormat != '') {
			    strBuilder.append(", dateFormat='${dateFormat}'")
			}                  
                  strBuilder.append(")");
                  return strBuilder.toString();                  
		}


		@ASTProducerMethod(processingType=ProcessingType.SOURCE, producedASTType=SparkScript.class, styleKey="ProcessingStyleKey.Spark.ExtractScript")
		AbstractSyntaxTree EXTRACT_TAP(SqlQuery sqlQuery) throws AdapterException,
        	                                             MappingException,
            	                                         GenerationException {

		supportDataFrames('LKM HDFS to Spark', true);

        	generatorContext.putObject( "mapping.spark.includeJdbcJars", new Boolean( true ) )
        	List<MapPhysicalNode> parentNodes = physicalNode.getUpstreamConnectedNodes();

        	if (parentNodes.size() != 1) {
            	throw new RuntimeException();
        	}

        	MapPhysicalNode parentNode = parentNodes.get(0);
        	Object srcCompBondObj = parentNode.getLogicalComponent().getBoundObject();
        	String techno_type = parentNode.getExecutionTechnology().getInternalName();
        	String storageFn = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "Storage Function");
        	String inputFormatClass = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "InputFormatClass");
        	String options = getDelimDataStoreStorageValue(parentNode, physicalNode);
        	String storage_FileFormat = null;

        	Boolean isStreamingMode = SparkScript.isNodeInStreamingMode(parentNode);
		//Boolean isStreamingMode = physicalNode.getPhysicalDesign().isStreaming();

		if ("HDFS".equals(techno_type)){
			storage_FileFormat = (((OdiDataStore) srcCompBondObj).getRealObject()).getHadoopStorageDescriptor().getFormatType().getName().toLowerCase()
        		if ("json".equals(storage_FileFormat))
        			storageFn = "json";
        		else if ("avro".equals(storage_FileFormat)){
        			storageFn = "com.databricks.spark.avro";}
        		else if ("delimited".equals(storage_FileFormat)){
                       		if (isStreamingMode)
					storageFn = "textFileStream";
				else
         				storageFn = "csv";}
        		else
        			storageFn = storage_FileFormat;
        		
        	}           	
        	if (isStreamingMode) {
        	   if (!"json".equals(storage_FileFormat) && !"delimited".equals(storage_FileFormat)) {
            		throw new OdiKMException(null, "ODIKM-SPARK-HDFS-10001","ODIKM-SPARK-HDFS-10001:Streaming is not supported for FileFormat "+storage_FileFormat+" in 'LKM HDFS to Spark' requires to uncheck Streaming", null);
            	   }
        	}       	

        	if (storageFn.equals("")) {
            	throw new MappingException("The option 'Storage Function' requires a value");
        	}
        	
        	String streamingContext = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "streamingContext");
            LKMSpark.LKMSparkStorage lkmSpark = getLKMSpark(storageFn);  
            lkmSpark.setPhysicalNodes(physicalNode, parentNode);
            
        	String dataSource = OdiRef.getOdiGeneratedAccessName("SOURCE_NAME", parentNode);
        	
        	TemplateUtils templateUtils = getTemplateUtils();
        	SparkLoadCmd loadCmd = new SparkLoadCmd(templateUtils, parentNode);
        	
        	if (lkmSpark.getBaseUrl() != null) {
		  dataSource = lkmSpark.getBaseUrl() + dataSource;
        	}
        	
        	def inferSourceSchema = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "inferSchema");
		loadCmd.setInferSourceShema(inferSourceSchema.equalsIgnoreCase("true") ? true : false)

		if ("HDFS".equals(techno_type)){
        		loadCmd.setTechnology(techno_type);
        		if ("avro".equals(storage_FileFormat)){
        			loadCmd.setFileFormat("AVRO");   
        		} else if ("parquet".equals(storage_FileFormat)){
        			loadCmd.setFileFormat("PARQUET");   
        		} else if ("json".equals(storage_FileFormat)){
        			loadCmd.setFileFormat("JSON");
        			def dateFormat = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "dateFormat")
        			if (dateFormat != null && dateFormat != '') {
				  loadCmd.setLoadOptions("options(dateFormat='${dateFormat}')");
        			}        			        			
        		} else if ("delimited".equals(storage_FileFormat)){
                   		if (isStreamingMode) {
                       	        	loadCmd.setKeySupport(false);
				} else {
        				loadCmd.setLoadOptions(options);
        			}
                       	        if (srcCompBondObj != null && srcCompBondObj instanceof OdiDataStore ) {
                       	             	 OdiDataStore srcTable = ((OdiDataStore) srcCompBondObj).getRealObject();
                       	                 if (srcTable.getFileDescriptor() != null){
                       	                       	 loadCmd.setFieldSep(SnpsStringTools.escapeUnicode(SnpsStringTools.hexStrToStr(SnpsStringTools.replaceNVL(srcTable.getHadoopStorageDescriptor().getFieldSeparator())),true));
                       	                         loadCmd.setRowSep(SnpsStringTools.escapeUnicode(SnpsStringTools.hexStrToStr(SnpsStringTools.replaceNVL(srcTable.getHadoopStorageDescriptor().getRowSeparator())),true));
                       	                }
                       	        }
        		}    
		}
 
        	if ("newAPIHadoopRDD".equals(storageFn))
			loadCmd.setLoadFile(false);
					
        	loadCmd.setFlexField("STREAMING_CONTEXT",streamingContext)
			
        	lkmSpark.addCommand(loadCmd);
        	lkmSpark.populateLoaderFunctionParams(loadCmd);
        	loadCmd.setFileLoc(dataSource);
        	loadCmd.setNeedFormatOutput(true)
        	

  	
        	SparkScript sparkScript = new SparkScript(templateUtils, null);
        	        	
        	sparkScript.addChild(loadCmd);
        	return sparkScript;
        
        	}

		@ASTProducerMethod(processingType=ProcessingType.TARGET, producedASTType=SparkScript.class, styleKey="ProcessingStyleKey.Spark.LoadScript")
		AbstractSyntaxTree LOAD_TAP(SparkScript sparkScript) throws AdapterException, MappingException, GenerationException {

        	supportDataFrames('LKM Spark to HDFS', true);

        	generatorContext.putObject( "mapping.spark.includeJdbcJars", new Boolean( true ) )
        	List<MapPhysicalNode> parentNodes = physicalNode.getUpstreamConnectedNodes();

        	List<MapPhysicalNode> childNodes = physicalNode.getDownstreamConnectedNodes();

        	MapPhysicalNode childNode = childNodes.get(0);
        	String techno_type = childNode.getExecutionTechnology().getInternalName(); 
        	Object tgtCompBondObj = childNode.getLogicalComponent().getBoundObject();
        	String options = getDelimDataStoreStorageValue(childNode, physicalNode);
        	String storage_FileFormat = null;
		Boolean isStreamingMode = physicalNode.getPhysicalDesign().isStreaming();
        	if (parentNodes.size() != 1) {
            	throw new RuntimeException();
        	}
        	
	      String storageFn = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "Storage Function");
		String outputFormatClass = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "OutputFormatClass");
		storage_FileFormat = (((OdiDataStore) tgtCompBondObj).getRealObject()).getHadoopStorageDescriptor().getFormatType().getName().toLowerCase();
		if ("HDFS".equals(techno_type)){        				
        		if ("json".equals(storage_FileFormat))
        		storageFn = "json"; 
        		else if ("avro".equals(storage_FileFormat))
        		storageFn = "com.databricks.spark.avro";
        		else if ("delimited".equals(storage_FileFormat)){
				if (isStreamingMode)
					storageFn = "csv";
				else
         				storageFn = "csv";}
        		else
        		storageFn = storage_FileFormat;
        	}    

        	if (storageFn.equals("")) {
            	throw new MappingException("The option 'Storage Function' requires a value");
        	}

            LKMSpark.LKMSparkStorage lkmSpark = getLKMSpark(storageFn); 
            lkmSpark.setPhysicalNodes(physicalNode, childNode);

        	MapPhysicalNode parentNode = lkmSpark.getNonAPParentNode(parentNodes.get(0));

        	String outputLocation = OdiRef.getOdiGeneratedAccessName("TARG_NAME", childNode,  "D");        
        
            if (lkmSpark.getBaseUrl() != null) {
    		   	outputLocation = lkmSpark.getBaseUrl() + outputLocation;
    		}
    		
            SparkStoreCmd storeCmd = new SparkStoreCmd(sparkScript.getTemplateUtils(), childNode);
            def inferSourceSchema = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "inferSchema");
            def sampleRatio = LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "SAMPLING_RATIO") == null ? 0 : Integer.valueOf(LKMSpark.LKMSparkStorage.getOptionValue(physicalNode, "SAMPLING_RATIO"));
            storeCmd.setSampleRatio(sampleRatio)
            storeCmd.setInferSourceSchema(inferSourceSchema.equalsIgnoreCase("true") ? true : false)
            storeCmd.setDirectory(outputLocation);
		if ("HDFS".equals(techno_type)){        				
        		storeCmd.setTechnologyType(techno_type);
        	}
            	if ("delimited".equals(storage_FileFormat)){
			storeCmd.setStoreOptions(options);
             		/*if (!isStreamingMode)
        			storeCmd.setStoreOptions(options);   
			else {
				storeCmd.setTechnologyType("");
                        	if (tgtCompBondObj != null && tgtCompBondObj instanceof OdiDataStore ) {
                                	OdiDataStore tgtTable = ((OdiDataStore) tgtCompBondObj).getRealObject();
                                	if (tgtTable.getHadoopStorageDescriptor() != null){
                                        	storeCmd.setFieldSep(SnpsStringTools.escapeUnicode(SnpsStringTools.hexStrToStr(SnpsStringTools.replaceNVL(tgtTable.getHadoopStorageDescriptor().getFieldSeparator())),true));
                                        	storeCmd.setRowSep(SnpsStringTools.escapeUnicode(SnpsStringTools.hexStrToStr(SnpsStringTools.replaceNVL(tgtTable.getHadoopStorageDescriptor().getRowSeparator())),true));
                                	}
                        	}
			}*/
        	}

            lkmSpark.addCommands(storeCmd);
            
        	lkmSpark.populateStorerFunctionParams(storeCmd);
        
        	storeCmd.setTgtAttrExprs(lkmSpark.getMapExpressions(childNode));
        	storeCmd.setTgtAttrNames(lkmSpark.getMapExpressionAliases(childNode));
        	storeCmd.setFullTgtAttrs(childNode.getNodeAttributes());
		storeCmd.setTgtDataTypes(lkmSpark.getConvertedTgtDataTypes(childNode,storeCmd.getTemplateUtils().getTechno()));
		
		storeCmd.setUseSQLExpressions(getOptionValue("SQL_EXPRESSIONS").equalsIgnoreCase("true") ? true : false)

        	List<SparkScript> sparkScripts = new ArrayList<SparkScript>(1);
        	sparkScripts.add(sparkScript);
        	SparkScript storeScript = new SparkScript(sparkScript.getTemplateUtils(), sparkScripts);             
        	storeScript.addChild(storeCmd);
        	
	      return getLKMStoreScript(storeScript);
		}
		
		@ASTProducerMethod(processingType=ProcessingType.TARGET, producedASTType=NullTarget.class,styleKey="ProcessingStyleKey.Spark.LoadScript")
		AbstractSyntaxTree PRE_EXTRACT_TAP(SqlQuery input) {
       		NullTarget result = new NullTarget();
			result.setUpstreamAST(input);
			return result;
		}
 
 	}

	]]></Field>
	<Field name="ExpectedAstClass" type="java.lang.String">null</Field>
	<Field name="ExtVersion" type="java.lang.String">null</Field>
	<Field name="FirstDate" type="java.sql.Timestamp"><![CDATA[2019-05-17 22:11:24.0]]></Field>
	<Field name="FirstUser" type="java.lang.String"><![CDATA[SUNOPSIS_INSTALL]]></Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[33920E0F49656934E053E603C40A3C0A]]></Field>
	<Field name="IndChange" type="java.lang.String"><![CDATA[U]]></Field>
	<Field name="IndExcludeExUnitBegin" type="java.lang.String">null</Field>
	<Field name="IndExcludeExUnitEnd" type="java.lang.String">null</Field>
	<Field name="IndExcludeExUnitMain" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapBegin" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapCleanup" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapEnd" type="java.lang.String">null</Field>
	<Field name="IndGenerateLoad" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IndIsHidden" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IndJrnMethod" type="java.lang.String">null</Field>
	<Field name="IndSuppSetBased" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IntgType" type="java.lang.String">null</Field>
	<Field name="IntVersion" type="com.sunopsis.sql.DbInt"><![CDATA[154]]></Field>
	<Field name="IsConcurrent" type="java.lang.String">null</Field>
	<Field name="IsSeeded" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IBaseCompKm" type="com.sunopsis.sql.DbInt"><![CDATA[134]]></Field>
	<Field name="IFolder" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IProject" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScBaseTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScOrigTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ITrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
	<Field name="ITxtDelTxt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ITxtTrtTxt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="KimMultiDserver" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KmDefault" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KmLang" type="java.lang.String">null</Field>
	<Field name="KmSrcLang" type="java.lang.String"><![CDATA[PYTHON]]></Field>
	<Field name="KmSrcTechno" type="java.lang.String"><![CDATA[SPARK_PYTHON]]></Field>
	<Field name="KmTechno" type="java.lang.String"><![CDATA[SPARK_PYTHON]]></Field>
	<Field name="KmVersion" type="java.lang.String">null</Field>
	<Field name="LastDate" type="java.sql.Timestamp"><![CDATA[2019-10-24 19:34:36.0]]></Field>
	<Field name="LastUser" type="java.lang.String"><![CDATA[SUPERVISOR]]></Field>
	<Field name="LkmType" type="java.lang.String"><![CDATA[N]]></Field>
	<Field name="LChecksum" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="LCode" type="java.lang.String">null</Field>
	<Field name="OggJkm" type="java.lang.String">null</Field>
	<Field name="OrdFolder" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ProcType" type="java.lang.String"><![CDATA[N]]></Field>
	<Field name="ProdAstType" type="java.lang.String">null</Field>
	<Field name="RepGuid" type="java.lang.String">null</Field>
	<Field name="RepVersion" type="java.lang.String">null</Field>
	<Field name="ScriptPath" type="java.lang.String">null</Field>
	<Field name="ScOrigTrtTag" type="java.lang.String">null</Field>
	<Field name="Subtype" type="java.lang.String"><![CDATA[SELECT*]]></Field>
	<Field name="TrtName" type="java.lang.String"><![CDATA[LKMSparkHDFS]]></Field>
	<Field name="TrtType" type="java.lang.String"><![CDATA[CK]]></Field>
	<Field name="VariableDefs" type="java.lang.String"><![CDATA[

		]]></Field>
	<Field name="VLastDate" type="java.sql.Timestamp">null</Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpReferencedObject">
		<Field name="IObject" type="com.sunopsis.sql.DbInt"><![CDATA[3600]]></Field>
	<Field name="ObjectPKasString" type="java.lang.String"><![CDATA[134]]></Field>
	<Field name="ObjectAKasString" type="java.lang.String"><![CDATA[]]></Field>
	<Field name="Description" type="java.lang.String"><![CDATA[SNP_TRT : LKMSpark]]></Field>
 <Field name="GlobalId" type="java.lang.String"><![CDATA[80aa8be2-d68d-40fe-bc1e-0b0b3ee0105e]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[8b1dd34d-173b-4e83-9ec3-8a044231a6eb]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[151]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[c4ad5509-f3cf-430e-bd9e-ffa3ef928021]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[149]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[aba65b64-7620-4b3b-b119-0c5cded11b5a]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[92]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[5e2b0ddc-c49a-4545-bffd-92edc6938119]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[150]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[9b4cdebd-a591-437f-890d-5af128b978c4]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[152]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[138]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.134]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[80aa8be2-d68d-40fe-bc1e-0b0b3ee0105e]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKMSpark]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[8]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.138]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[33920E0F49656934E053E603C40A3C0A]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[LKMSparkHDFS]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[12]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.151]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46e0aa-2d00-11e6-93fc-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmitAsync]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[22]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.149]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[bcb3a042-a6d7-11e6-ac06-23e5aaa09e30]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmitCommon]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[23]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.92]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46e05a-2d00-11e6-93fa-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmit]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[17]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.150]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46e082-2d00-11e6-93fb-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonSubmitSync]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[21]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.152]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[bc17c799-d8eb-4fe2-9440-a78c825b817b]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SparkPythonRESTSubmit]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[21]]></Field>
</Object>
<Object class="com.sunopsis.dwg.DwgExportSummary">
		<Field name="ExpTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="InstObjNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="LinkDiagNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MorigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MtxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OrigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OtherObjectsNb" type="com.sunopsis.sql.DbInt"><![CDATA[6]]></Field>
	<Field name="PlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="StepNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="TxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="UeOrigNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="UeUsedNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="VarPlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="ScenTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OdiVersion" type="java.lang.String"><![CDATA[12.2.1]]></Field>
	<Field name="OriginRepositoryID" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="RepositoryVersion" type="java.lang.String"><![CDATA[05.02.02.08]]></Field>
</Object>
</SunopsisExport>
