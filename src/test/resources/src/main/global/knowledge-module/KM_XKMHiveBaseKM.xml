<?xml version="1.0" encoding="ISO-8859-1"?>
<SunopsisExport>
<Admin RepositoryVersion="05.02.02.08" OdiVersion="12.2.1" IsLegacyIdCompatible="false" IsExportWithParents="true" />
<Encryption algorithm="AES" keyLength="128" exportKeyHash="7IbzFdMPlabvZU7sWJgnV34Nxl7PvAc/13vRpq4uOoo=" keyVect="FFGawQu0VUc+uuFbxTTXHA==" exportKeySalt="5f9a2f3b-c1ed-458b-b6cd-41efa9de4fd5" containsCipherText="false"/>
<SmartExportList materializeShortcut="false" >
   <Include><![CDATA[SnpTrt#53]]></Include>
</SmartExportList>
<Object class="com.sunopsis.dwg.dbobj.SnpTrt">
		<Field name="CleanupOnError" type="java.lang.String">null</Field>
	<Field name="CompType" type="java.lang.String"><![CDATA[DATASTORE]]></Field>
	<Field name="DelegateClass" type="java.lang.String"><![CDATA[HiveBaseKM]]></Field>
	<Field name="DelegateScript" type="java.lang.String"><![CDATA[
        import java.util.regex.Matcher
    	  
    	import oracle.odi.domain.topology.OdiTechnology
    	import oracle.odi.domain.topology.OdiDataServer
    	import oracle.odi.domain.mapping.MappingUtils
    	import oracle.odi.domain.adapter.relational.IColumn
    	import oracle.odi.domain.odireference.OdiDataSetRef
    	import oracle.odi.domain.model.OdiColumn
    	import oracle.odi.domain.model.OdiDataStore
    	import oracle.odi.mapping.generation.Column
    	import oracle.odi.mapping.generation.Table
    	import oracle.odi.domain.adapter.project.IProcedureLine
    	import oracle.odi.domain.adapter.topology.finder.IOdiFlexFieldFinder
    	import oracle.odi.domain.mapping.xreference.MapExpressionXRef
    	import oracle.odi.domain.mapping.MapAttribute
    	import oracle.odi.domain.adapter.project.IKnowledgeModule
    	import oracle.odi.adapter.OdiAdapter
    	import oracle.odi.domain.topology.OdiFlexField
    	import java.util.concurrent.atomic.AtomicInteger
    	import oracle.odi.km.exception.OdiKMException
    	import oracle.odi.domain.mapping.exception.MappingException
    	import oracle.odi.domain.mapping.exception.RMUnconnectedInputException
    	import oracle.odi.mapping.generation.JoinTable  
    	import oracle.odi.mapping.generation.SetQuery  	
    	import com.sunopsis.tools.core.SnpsStringTools
    	
    	import oracle.odi.domain.adapter.project.IProcedureLine
    	import oracle.odi.adapter.OdiLocationAdapter    
    	import oracle.odi.domain.topology.OdiLogicalSchema	
    	import oracle.odi.domain.mapping.physical.MapPhysicalNode
    	import oracle.odi.domain.mapping.physical.MapPhysicalExpression
    	import oracle.odi.domain.mapping.MapAttribute
    	import oracle.odi.domain.mapping.expression.MapExpression

    	import org.codehaus.jackson.JsonNode;
        import org.codehaus.jackson.JsonParseException;
        import org.codehaus.jackson.map.JsonMappingException;
        import org.codehaus.jackson.map.ObjectMapper;
        import oracle.odi.mapping.generation.NullTarget;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.ASTProducerMethod;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.SubstitutionVariables;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.SubstitutionVariable;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.SubstitutionAPIMethod;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.UsesTemplate;
        import oracle.odi.mapping.generation.AbstractSyntaxTree.ASTClass;
        import oracle.odi.domain.mapping.generator.KMGeneratorDelegateRegistry.GeneratorDelegateClass;
        import oracle.odi.mapping.generation.parser.TemplateParserBase;

    	////////////////////////////////////////////
    	// Groovy MOP section
    	////////////////////////////////////////////
    	
    	// Define a few extra methods for Class OdiDataStore 
    	// Refreshing of closures works fine here!
    	    //TODO: Cleanup
    	    OdiDataStore.metaClass.getJavaEscapedFieldSeparator = 
    	      { return /*org.apache.commons.lang.StringEscapeUtils.escapeJava(*/ delegate.getFileDescriptor().getFieldSeparator() /*)*/
/*    	                                                        .replaceAll( java.util.regex.Pattern.quote('\\'), java.util.regex.Matcher.quoteReplacement('\\<$ $>') )
    	                                                        .replaceAll( java.util.regex.Pattern.quote('\\\\  '), java.util.regex.Matcher.quoteReplacement('\\<$ $>') )*/ }
    	    OdiDataStore.metaClass.getJavaEscapedRowSeparator = 
    	      { return /*org.apache.commons.lang.StringEscapeUtils.escapeJava(*/ delegate.getFileDescriptor().getRowSeparator() /*)*/
    	                                                        .replaceAll( java.util.regex.Pattern.quote("\\u000A"), java.util.regex.Matcher.quoteReplacement("\\n") ) }
    	    OdiDataStore.metaClass.isDelimited = 
    	      // For some reason the fully qualified class below must stay fully qualified. Otherwise Groovy compilation exception.
    	      { return (delegate.getFileDescriptor().getFormat().equals( oracle.odi.domain.model.OdiDataStore$FileDescriptor$Format.DELIMITED )) }
    	    OdiDataStore.metaClass.getColList = 
    	      { String p1, String p2 -> return delegate.getColumns().collect( { col -> col.getName() + ":" + col.getFileFieldDescriptor().getBytes() } ) }
    	      //< %=odiRef.getSrcColList("", "(.{[BYTES]})", "(.{[BYTES]})", "", "")% >    	      

            //TODO: for some reason metaClasses are only reset after ODI restart. The lines below do NOT solve this issue!
            // Is all of this still true?
            GroovySystem.metaClassRegistry.removeMetaClass (Column)
            GroovySystem.metaClassRegistry.removeMetaClass (Column.class)
        	GroovySystem.metaClassRegistry.removeMetaClass Column
        	Column.metaClass = null
        	Column.metaClass.initialize()

            java.util.List.metaClass.collectWithIndex = { c ->
            	def i=0
            	delegate.collect { c(it, i++) }
            }
        	
        	// Add three parameter join method to AbstractList
        	java.util.AbstractList.metaClass.join = { String sBegin, String sSep, String sEnd ->
        	    def s = delegate.join( sSep )
        	    if ( s.isEmpty() ) return ""
        	    return sBegin + s + sEnd
        	}
        	
        	/* Access to DataServer FFs must happen using ?-level code
        	OdiDataServer.metaClass.getFlexFieldValue = { String ffCode ->
                delegate.initFlexFields( HiveBaseKM.ffFinder )
                def ffs = delegate.getFlexFieldsValues()
                def ff  = ffs.find( {it.getCode().equals( ffCode )} )
                return ff?.getValue()
        	}
        	*/


        	Table.metaClass.initFlexFields = { IOdiFlexFieldFinder ffFinder ->
                def dataStore = delegate.getModelTable()?.getBoundDataStore()
                if ( dataStore != null ) 
                    dataStore.initFlexFields( ffFinder )
        	}

                Table.metaClass.getFlexFieldValue = { String ffCode ->
                def dataStore = delegate.getModelTable()?.getBoundDataStore()
                if ( dataStore == null ) return null
                //dataStore.initFlexFields( HiveBaseKM.ffFinder )
                def ffs = dataStore.getFlexFieldsValues()
                def ff  = ffs.find( {it.getCode().equals( ffCode )} )
                return ff?.getValue()
                }

                Table.metaClass.getHiveBuckets = {
                def dataStore = delegate.getModelTable()?.getBoundDataStore()
                if ( dataStore == null ) return null
                if (dataStore.getHadoopStorageDescriptor() != null)
                    return dataStore.getHadoopStorageDescriptor().getHiveBuckets()
                else
                    return null;
                }

                OdiDataStore.metaClass.getFlexFieldValue = { String ffCode ->
                def dataStore = delegate
                if ( dataStore == null ) return null
                //dataStore.initFlexFields( HiveBaseKM.ffFinder )
                def ffs = dataStore.getFlexFieldsValues()
                def ff  = ffs.find( {it.getCode().equals( ffCode )} )
                return ff?.getValue()
                }

                OdiDataStore.metaClass.getHiveBuckets = {
                def dataStore = delegate
                if ( dataStore == null ) return null
                if (dataStore.getHadoopStorageDescriptor() != null)
                    return dataStore.getHadoopStorageDescriptor().getHiveBuckets()
                else
                    return null;
                }

        	OdiDataStore.metaClass.getColumns = { IOdiFlexFieldFinder ffFinder ->
                def cols = delegate.getColumns()
                cols.each { col -> col.initFlexFields( ffFinder ) }
                return cols
        	}

                OdiColumn.metaClass.getFlexFieldValue = { String ffCode ->
                def col = delegate
                //col.initFlexFields( HiveBaseKM.ffFinder )
                def ffs = col.getFlexFieldsValues()
                def ff  = ffs.find( {it.getCode().equals( ffCode )} )
                return ff?.getValue()
                }

                OdiColumn.metaClass.isCluster = {
                  def col = delegate
                  return col.isUsedForClustering()
                }
                
                OdiColumn.metaClass.isPartition = {
                  def col = delegate
                  return col.isUsedForPartitioning()
                }

                OdiColumn.metaClass.isSort = {
                  def col = delegate
                  Integer sortByPos = col.getSortByPosition();
                  if (sortByPos == null || sortByPos.intValue() <= 0)
                    return false;
                  else
                    return true;
                }
  
                OdiColumn.metaClass.getSortDirection = {
                   def col = delegate
                   return col.getSortByDirection().toString()
                }

                OdiColumn.metaClass.getSortPosition = {
                   def col = delegate
                   return col.getSortByPosition().intValue()
                }

        	// Some default implementations, e.g. for handling OdiDataSetRef$VirtualColumn
        	IColumn.metaClass.isPartition = { false }
        	IColumn.metaClass.isCluster   = { false }
        	IColumn.metaClass.isSort      = { false }
        	IColumn.metaClass.getSortDirection      = { "ASC" }
        	IColumn.metaClass.getSortPosition      = { 0 }
        	IColumn.metaClass.getRefObjectOrPhysColumn = { return (delegate.metaClass.respondsTo( delegate, "getRefObject" ))?delegate.getRefObject():null }
        	 
        	// Column object cannot reference a MapPhysicalColumn (like for APs), as 
        	// refObject must implement IModelObject and MapPhysicalColumn is a non-persistent 
        	// codegen artifact and therefore cannot be used here.
        	// The metaClass entries below extend the Column class to handle references
        	// both to IModelObject and MapPhysicalColumn. This way KM code can be
        	// simplified, as distinction between AP and non-AP columns is no longer
        	// required.
        	Column.metaClass.mRefPhysColumn = null
        	Column.metaClass.setPhysColumn = { obj ->
        		delegate.mRefPhysColumn = obj
        	}
        	def savedColumn_getColumn = Column.metaClass.getStaticMetaMethod('getColumn', [Table, MapPhysicalColumn] as Class[])
        	Column.metaClass.static.getColumn = { Table tab, MapPhysicalColumn physCol ->
        	    // Call original method
        	    def col = savedColumn_getColumn.doMethodInvoke( null, tab, physCol )
        	    // Store the physical Column
        	    col.setPhysColumn( physCol )
        	    return col
        	}
        	Column.metaClass.getRefObjectOrPhysColumn = { 
        	    return (delegate.mRefPhysColumn!=null)?delegate.mRefPhysColumn:delegate.getRefObject()
        	}
        	Column.metaClass.mDdlDataType = null
        	Column.metaClass.setDdlDataType = { String dt ->
        		delegate.mDdlDataType = dt
        	}

        	Column.metaClass.initFlexFields = { IOdiFlexFieldFinder ffFinder ->
        	    def col = delegate.getRefObject()
        		if ( !(col instanceof OdiColumn) ) {
        		    def mapAttr = delegate.getRefObject()
        		    col = mapAttr.getBoundColumn()
        		}
        		// If this Column is not bound to any model column, then there is no FlexField defined and thus no value
        		if (col != null) 
        		    col.initFlexFields( ffFinder )        	
            }
        	
                // Convenience method to fetch FlexField information from referenced object
                Column.metaClass.getFlexFieldValue = { String ffCode ->
                        def col = delegate.getRefObject()
                        if ( !(col instanceof OdiColumn) ) {
                            def mapAttr = delegate.getRefObject()
                            col = mapAttr.getBoundColumn()
                        }
                        // If this Column is not bound to any model column, then there is no FlexField defined and thus no value
                        if (col == null) 
                            return null
                // col.initFlexFields( HiveBaseKM.ffFinder )
                def ffs = col.getFlexFieldsValues()
                def ff  = ffs.find( {it.getCode().equals( ffCode )} )
                return ff?.getValue()
                }

                Column.metaClass.getOdiColumn = { 
                        def col = delegate.getRefObject()
                        if ( !(col instanceof OdiColumn) ) {
                            def mapAttr = delegate.getRefObject()
                            col = mapAttr.getBoundColumn()
                        }
                        return col
                }

                Column.metaClass.isCluster = {
                def odiCol = delegate.getOdiColumn()
                return (odiCol == null)?
                        // Check table object for column metadata
                        delegate.getTable().isClusterColumn( delegate ) :
                        // Otherwise use the column's FF value
                        odiCol.isUsedForClustering()
                }
                
                Column.metaClass.isPartition = {
                def odiCol = delegate.getOdiColumn()
                return (odiCol == null)?
                        // Check table object for column metadata
                        delegate.getTable().isPartitionColumn( delegate ) :
                        // Otherwise use the column's FF value
                        odiCol.isUsedForPartitioning()
                }
                
                Column.metaClass.isSort = {
                def odiCol = delegate.getOdiColumn()
                return (odiCol == null)?
                        // Check table object for column metadata
                        delegate.getTable().isSortColumn( delegate ) :
                        // Otherwise use the column's FF value
                        odiCol.getSortByPosition() != null
                }
	
                Column.metaClass.getSortDirection = {
                def odiCol = delegate.getOdiColumn()
                return (odiCol == null)?
                        "ASC" :
                        // Otherwise use the column's FF value
                        odiCol.getSortByDirection().toString()
                }

                Column.metaClass.getSortPosition = {
                def odiCol = delegate.getOdiColumn()
                return (odiCol == null)?
                        0 :
                        // Otherwise use the column's FF value
                        odiCol.getSortByPosition().intValue()
                }
                
            Column.metaClass.isMapped = { 
            	return (delegate.getRefObjectOrPhysColumn()?.getExpression() != null && 
                	    (!(delegate.getRefObjectOrPhysColumn()?.getExpression() instanceof UnmappedColumnExpression)) ) 
            }
            Column.metaClass.referencesUpstreamAttribute = { 
                // Get list of referenced attributes (exclude all variable and sequence references)            	
            	def refAttrs = delegate.getRefObjectOrPhysColumn()?.getExpression()?.getCrossReferences()?.findAll { MapExpressionXRef xref -> return xref.isAttributeReference() }
                return (refAttrs != null && !refAttrs.isEmpty()) 
            }
        	
            Column.metaClass.isConstantTargetExpression = { 
            	def hasMultipleExpressions = (delegate.getRefObjectOrPhysColumn()?.getExpressions().size() > 0)
                return !referencesUpstreamAttribute() && !hasMultipleExpressions
            }
        	
            MapAttribute.metaClass.isMapped = { 
            	return (delegate.getExpression() != null && 
                	    (!(delegate.getExpression() instanceof UnmappedColumnExpression)) ) 
            }
            
            /**
             * getSQLAccessName returns an optionally double-quoted identifier.
             * @techno Technology for which the quotation rules should be applied
             * @name name of unquoted identifier
             */
            MapAttribute.metaClass.static.getSQLAccessName = { OdiTechnology techno, String inputName, boolean forceDoubleQuoting = false ->
                String attrBoundObjName = inputName
                /* Copied from MapAttribute.getSQLAccessName */
                // add delimiters to column name if necessary, refer to current implementation in SnpLanguage
                ILanguage language = techno.getDefaultLanguageImpl();
                if (language != null) {
                  char[] delimiters = language.getObjectDelimeters();
                  
                  String name = techno.getInternalName().equalsIgnoreCase("POSTGRESSQL") ? 
                    attrBoundObjName.toLowerCase(Locale.ENGLISH) : attrBoundObjName.toUpperCase(Locale.ENGLISH);
                  boolean isCaseSensitive = language.isObjectNamesCaseSensitive();
                  boolean containWordSep = false;
                  
                  String wordSep = "";
                  char[] wordSepChars = language.getWordSeparators();
                  if (wordSepChars != null) { 
                    wordSep = String.valueOf(wordSepChars);
                  }
                  wordSep += " \t\r\n";  // default word separators
                  for (int c : wordSep.toCharArray()) {
                    if (attrBoundObjName.indexOf( c ) >= 0) {
                      containWordSep = true;
                      break;
                    }
                  }
                
                  // Added to handle HBase's key column. 
                  // Required as there is no double-quoting for reserved words.
                  def isKeyword = ["key"].contains( attrBoundObjName )
                  
                  if (containWordSep || isKeyword || forceDoubleQuoting || (!name.equals(attrBoundObjName) && !isCaseSensitive)) {
                    if (delimiters != null && delimiters.length == 2) {
                      attrBoundObjName = "" + delimiters[0] + attrBoundObjName + delimiters[1];
                    }
                  }
                }
                return attrBoundObjName            
            }
            
           /**
            * Get a SQL or code generation alias name for this source attribute.   This will be the 
            * attribute name, possibly modified if special characters are found.            
            * Copied from MapAttribute.getSQLAliasName
            * @techno Technology to be used for calculating the alias
            * @return A SQL alias name for the source attribute.
            */
            MapAttribute.metaClass.getSQLAliasName = { OdiTechnology techno ->                  
                  String result = "";                  
                  String attrName = delegate.getName();
                  
                  // add delimiters to column name if necessary, refer to current implementation in SnpLanguage
                  ILanguage language = techno.getDefaultLanguageImpl();
                  if (language != null) {
                    char[] delimiters = language.getObjectDelimeters();
                    
                    String name = techno.getInternalName().equalsIgnoreCase("POSTGRESSQL") ? 
                      attrName.toLowerCase(Locale.ENGLISH) : attrName.toUpperCase(Locale.ENGLISH);
              //      boolean isCaseSensitive = language.isObjectNamesCaseSensitive();
                    boolean containsWordSep = false; 
                    
                    String wordSep = "";
                    char[] wordSepChars = language.getWordSeparators();
                    if (wordSepChars != null) { 
                      wordSep = String.valueOf(wordSepChars);
                    }
                    wordSep += " \t\r\n";  // default word separators
                    for (int c : wordSep.toCharArray()) {
                      if (attrName.indexOf(c) >= 0) {
                        attrName = attrName.replace((char)c, (char)'_');
                        containsWordSep = true;
                        //remove the break here since attr name may contain multi special character.
                        //break;
                      }
                    }
                    
                    if (containsWordSep) {
                      // Ensure uniqueness within set of attribute names, since we have substituted characters in the original unique attribute name.
                      attrName += "_" + delegate.getPosition().toString();
                    }
                  
                  }
                  
                  result += attrName;                  
                  return result;
            }          
            
            MapAttribute.metaClass.referencesUpstreamAttribute = { 
                // Get list of referenced attributes (exclude all variable and sequence references)
                def refAttrs = delegate.getExpression()?.getCrossReferences()?.findAll { MapExpressionXRef xref -> return xref.isAttributeReference() }
                return (refAttrs != null && !refAttrs.isEmpty()) 
            }
            
            // Returns a copy of the input query with columns added for any unmapped or constant expression mappings
            // Takes attribute list from component
            /*
            SqlQuery.metaClass.getExtendedQuery = { Closure forEachAttribute ->
                def query = new SqlQuery( delegate )
		//query.setUseSubselectForMultiJoin(true) // for join
                def selectList = query.getSelectList()
                def aliasList  = query.getAliasList()
                AtomicInteger idx = new AtomicInteger(0)
                def extendedSelectList = component.getAttributes().collect { attr -> forEachAttribute( attr, selectList, idx ) }
                if ( aliasList != null && aliasList.size() > 0 ) {
                    def extendedAliasList  = component.getAttributes().collect { attr -> return new StringExpression( attr.getSQLAliasName() ) }
                    query.setAliasList( extendedAliasList )
                }
                query.setSelectList( extendedSelectList )	
                return query
            }
            */
    	
            /**
             * This method returns the list of journalized source Datastore nodes for the current node.
             */
            /* Currently not needed, as generatorContext.getJRNInfo() provides access to journalization information
            MapPhysicalNode.metaClass.getUpstreamJournalizedSources = {
                if (!delegate.isAPNode())
                    throw new OdiKMException( "MapPhysicalNode.getUpstreamJournalizedSources() is only defined for AP nodes!" )
                MapPhysicalNode sourceNode   = delegate.getChildNodes()[0]
                ExecutionUnit   srcExUnit    = sourceNode.getExecutionUnit()
                List<MapPhysicalNode> result = new ArrayList<MapPhysicalNode>()
                delegate.getUpstreamJournalizedSources( result, srcExUnit )
                return result
            }            
            MapPhysicalNode.metaClass.getUpstreamJournalizedSources = { List<MapPhysicalNode> result, ExecutionUnit exUnit ->
                List<MapPhysicalNode> upstreamNodes = delegate.getUpstreamConnectedNodes();
                for (MapPhysicalNode upstreamNode : upstreamNodes) {
                    if (!exUnit.equals(upstreamNode.getExecutionUnit()))
                        continue        
                    if (upstreamNode.isSourceNode()) {
                        // Is "Journalized Data Only" set?
                        if ( DatastoreComponent.isJournalizedDatastoreNode( upstreamNode ) )
                            result.add(upstreamNode)
                        continue;
                    }
                    upstreamNode.getUpstreamAPNodes(result, exUnit);
                }
            }
            */

            /**
             * This method returns whether the node is a direct load node or not
             */
            MapPhysicalNode.metaClass.isDirectLoad = { delegate.getLKM()?.getLKMType() == IKnowledgeModule.LKMType.TRANSPARENT_TARGET }

            /**
             * @odiInternal
             * Get all physical expressions associated with this node.
             * Copied from MapPhysicalNode.getSortedTargetPhysicalExpressions
             * @return The list of expressions, sorted in the order of their owning attributes.
             */
            MapPhysicalNode.metaClass.getSortedAllPhysicalExpressions = {
              List<MapPhysicalExpression> result = getAllPhysicalExpressions();
              
              List<MapExpression> logExprs = null;
              MapConnectorPoint cp = delegate.getLogicalConnectorPoint();
              if (cp != null) {
              	try {
              		logExprs = cp.getAttributeExpressions();
              	} catch (Exception ex) {
              		ex.printStackTrace();
              	}
              }
              final List<MapExpression> sortedLogicalExpressions = logExprs;
              
              // Sort the physical expression in the order of the owning attributes.
              Collections.sort(result, new Comparator<MapPhysicalExpression>() {
          
          			@Override
                public int compare(MapPhysicalExpression o1, MapPhysicalExpression o2) {
          				if (sortedLogicalExpressions != null) {
          					try {
          						MapExpression logExpr1 = o1.getRefLogicalExpression();
          						MapExpression logExpr2 = o2.getRefLogicalExpression();
          						int index1 = sortedLogicalExpressions.indexOf(logExpr1);
          						int index2 = sortedLogicalExpressions.indexOf(logExpr2);
          						return Integer.valueOf(index1).compareTo(Integer.valueOf(index2));
          					} catch (Exception ex) {}
          				}
          	      return 0;
                }
              	
              });
              
              return result;
            }

            
            /**
             * The following dump2 methods provide a convenient way to dump out metadata about the respective object 
             * and its related objects. It is usually used with a println statement:
             *     println ("Debugging: ${physicalNode.dump2()}")
             * It is very handy when trying to find the way through physical/logical columns/attributes and expressions
             * @param indent String to be prefixed to any generated line
             */
            MapPhysicalNode.metaClass.dump2 = { String indent="" ->
                def node = delegate
                def sb = new StringBuffer()
                sb += "\n"
                sb += "Node ${node}\n"
                sb += "    name: ${node.getName()}\n"
                sb += "    NodeAttributes:\n${node.getNodeAttributes().collect{ it.dump2( indent+"    " ) }.join()}\n"
                sb += "    PhysicalColumns:\n${node.getPhysicalColumns().collect{ it.dump2( indent+"        " )}.join()}\n"
                sb += "    getAllPhysicalExpressions:\n${node.getAllPhysicalExpressions().collect{ it.dump2( indent+"        " ) }.join()}\n"
                sb += "    getTargetPhysicalExpressions:\n${node.getTargetPhysicalExpressions().collect{ it.dump2( indent+"    " ) }.join()}\n"
                sb += "    getSourcePhysicalExpressions:\n${node.getSourcePhysicalExpressions().collect{ it.dump2( indent+"    " ) }.join()}\n"
                return sb.toString()
        	}

            MapPhysicalColumn.metaClass.dump2 = { String indent="" ->
                def col = delegate
                def sb = new StringBuffer()
                sb += indent+ "\n"
                sb += indent+ "PhysColumn ${col}\n"
                sb += indent+ "    Name:                ${col.getName()}\n"
                sb += indent+ "    Owner:               ${col.getOwner()}\n"
                sb += indent+ "    RefObject:           ${col.getRefObject()}\n${col.getRefObject()?.dump2( indent+"        ")}"
                sb += indent+ "    ReferencedAttribute: ${getReferencedAttribute()}\n"
                sb += indent+ "    SourceAttribute():   ${getSourceAttribute()}\n"
                return sb.toString()
        	}

            MapPhysicalExpression.metaClass.dump2 = { String indent="" ->
                def expr = delegate
                def sb = new StringBuffer()
                sb += indent+ "\n"
                sb += indent+ "PhysExpression ${expr}\n"
                sb += indent+ "    Owner:               ${expr.getOwner()}\n"
                sb += indent+ "    RefObject:           ${expr.getRefObject()}\n"
                return sb.toString()
        	}

            MapAttribute.metaClass.dump2 = { String indent="" ->
                def attr = delegate
                def sb = new StringBuffer()
                sb += indent+ "\n"
                sb += indent+ "MapAttribute ${attr}\n"
                sb += indent+ "    Name:                ${attr.getName()}\n"
                sb += indent+ "    SQLAliasName:        ${attr.getSQLAliasName()}\n"
                sb += indent+ "    Owner:               ${attr.getOwner()}\n"
                sb += indent+ "    Expressions:         ${attr.getExpressions()}\n${attr.getExpressions()?.collect{ it.dump2( indent+"        ")}.join() }\n"
                return sb.toString()
        	}

            MapExpression.metaClass.dump2 = { String indent="" ->
                def expr = delegate
                def sb = new StringBuffer()
                sb += indent+ "\n"
                sb += indent+ "MapExpression ${expr}\n"
                sb += indent+ "    Owner:               ${expr.getOwner()}\n"
                sb += indent+ "    OwningAttribute:     ${expr.getOwningAttribute()}\n"
                //sb += indent+ "    RefObject:           ${expr.getRefObject()}\n"
                sb += indent+ "    ReferencedObjects:   ${expr.getReferencedObjects()}\n"
                sb += indent+ "    AllMapReferences:    ${expr.getAllMapReferences()}\n"
                sb += indent+ "    CrossReferenceMap:   ${expr.getCrossReferenceMap()}\n"
                return sb.toString()
        	}

            Column.metaClass.dump2 = { String indent="" ->
                def col = delegate
                def sb = new StringBuffer()
                sb += indent+ "\n"
                sb += indent+ "Column ${col}\n"
                sb += indent+ "    Name:         ${col.getName()}\n"
                sb += indent+ "    TableName:    ${col.getTableName()}\n"
                sb += indent+ "    RefObj:       ${col.getRefObject()}\n"
                sb += indent+ "    RefObjOrPhys: ${col.getRefObjectOrPhysColumn()}\n"                
                return sb.toString()
        	}



    	////////////////////////////////////////////
    	// AST and other global classes
    	////////////////////////////////////////////
  @ASTClass(descriptionKey="ASTClass.HiveSqlQuery")
  @SubstitutionVariables([
    @SubstitutionVariable(name="COMPLEX_ATTR_FULL_NAME",type=String.class),
    @SubstitutionVariable(name="OUTER_LATERAL_VIEW",type=String.class),
    @SubstitutionVariable(name="LATERAL_VIEW_TABLE_ALIAS",type=String.class),
    @SubstitutionVariable(name="LATERAL_VIEW_COLUMN_ALIAS",type=String.class)
  ])
  @UsesTemplate(name="LateralViewText")
  public class HiveSqlQuery extends SqlQuery {

//    private List<String> orderByOperatorList = null;
//    private List<ArrayExpression> orderByList2 = null;

    // Variables inroduced for flatten component
    private boolean isFlattenCollection    
    public static class FlattenedComplexColumn {
      private boolean isFlattenCollection;
      private String complexAttributeFullName;
      private String lvTabAlias;
      private String lvColAlias;
      private boolean includeNulls;
      AbstractSyntaxTree parentAST = null;  // used to access the templateUtils

      public FlattenedComplexColumn(AbstractSyntaxTree pParentAST, boolean pIsFlattenCollection, String pComplexAttributeFullName, String pLvTabAlias, String pLvColAlias, boolean pIncludeNulls) {
        parentAST = pParentAST;
        isFlattenCollection = pIsFlattenCollection;
        complexAttributeFullName = pComplexAttributeFullName;
        lvTabAlias = pLvTabAlias;
        lvColAlias = pLvColAlias;
        includeNulls = pIncludeNulls;
      }
      
      public boolean isFlattenCollection() {
        return this.isFlattenCollection;
      }
      public String getComplexAttributeFullName() {
        return complexAttributeFullName;
      }
      public String getLvTabAlias() {
        return lvTabAlias;
      }
      public String getLvColAlais() {
        return lvColAlias();
      }
      public boolean isIncludeNulls() {
        return includeNulls;
      }
      public String getText() {
          // example: LATERAL VIEW OUTER explode(recipe.ingredients) lvIngredients as cIngredients
        if (isFlattenCollection()) {
          TemplateUtils tmplUtils = parentAST.getTemplateUtils();
          String templateText = tmplUtils.getTemplateText("LateralViewText");
          Map<String,Object> substitutionMap = new LinkedHashMap<String,Object>();
          substitutionMap.put("OUTER_LATERAL_VIEW", includeNulls);
          substitutionMap.put("COMPLEX_ATTR_FULL_NAME", complexAttributeFullName);
            // these match XKM options LATERAL_VIEW_TAB_ALIAS and LATERAL_VIEW_COL_ALIAS, but the template substitution vars are spelled differently
          substitutionMap.put("LATERAL_VIEW_TABLE_ALIAS", lvTabAlias);
          substitutionMap.put("LATERAL_VIEW_COLUMN_ALIAS", lvColAlias);
          return TemplateParserBase.mainCodeParser(tmplUtils, substitutionMap, templateText);
        }
        else
          return "";
      }
    }
    
    private List<FlattenedComplexColumn> flattenedComplexColList = new ArrayList<FlattenedComplexColumn>();
    

    public HiveSqlQuery(SqlQuery otherQuery) {
      super(otherQuery);
    }

    public HiveSqlQuery(HiveSqlQuery otherQuery) {
      super(otherQuery);
      isFlattenCollection = otherQuery.isFlattenCollection()
      flattenedComplexColList.addAll(otherQuery.flattenedComplexColList);
      // this.orderByList2 = otherQuery.orderByList2.clone();
    }

    public HiveSqlQuery(TemplateUtils utils) {
      super(utils);
    }

    public SqlQuery cloneAST() {
      HiveSqlQuery result = new HiveSqlQuery(this);
      return result;
    }

    public SqlQuery clone() {
      HiveSqlQuery result = new HiveSqlQuery(this);
      return result;
    }

    public SqlQuery createQuery() {
      HiveSqlQuery result = new HiveSqlQuery(getTemplateUtils());
      //result.setUseSubselectForMultiJoin(true);
      initSourceLocation(result);
      return result;
    }

    public SqlQuery createInlineView (boolean optimize) {
      HiveSqlQuery ilv = (HiveSqlQuery)super.createInlineView(optimize);
      ilv.isFlattenCollection = isFlattenCollection;
      ilv.flattenedComplexColList.addAll(flattenedComplexColList);
      return ilv;
    }

    public SqlQuery getTargetQuery (List<MapPhysicalExpression> tgtExpressions) throws MappingException {
      def query = super.getTargetQuery(tgtExpressions);
      return query;
    }

    // Methods introduced for flatten component
    public boolean isFlattenCollection() {
      return this.isFlattenCollection
    }
    
    protected boolean hasFlattenedCollection() {
      return isFlattenCollection
    }
    
    public void addFlattenedComplexColumn(boolean pIsFlattenCollection, String pComplexAttributeFullName, String pLvTabAlias, String pLvColAlias, boolean pIncludeNulls) {
      FlattenedComplexColumn flattenedCol = new FlattenedComplexColumn(this, pIsFlattenCollection, pComplexAttributeFullName, pLvTabAlias, pLvColAlias, pIncludeNulls);
      flattenedComplexColList.add(flattenedCol);
      isFlattenCollection = true;
    }

    public List<FlattenedComplexColumn> getFlattenedComplexColList() {
      return flattenedComplexColList;
    }
    
    /**
    * Gets the flattened complex columns that would result in a lateral view, basically only those that comprise a collection.
    * This method would be used by the Hive template component FromText.
    */
    public List<FlattenedComplexColumn> getLateralViewList() {
      List<FlattenedComplexColumn> result = new ArrayList<FlattenedComplexColumn>();
      for (FlattenedComplexColumn fcc : flattenedComplexColList) {
        if (fcc.isFlattenCollection())
          result.add(fcc);
    }
      return result;
    }
    
     /**************************************************************
     * Get the various SORT BY/CLUSTER BY/DISTRIBUTE BY/ORDER BY expressions of this query.
     *
     * @return The group by expressions.
     **************************************************************/
    public String getOrderByOperator()
    {
      return getPropertyValue("orderByOperatorList");
    }

    ///////////////////////////////////////////////////////////////////
    //
@Override
    public List<ArrayExpression> getOrderByList()
    {
	return super.getOrderByList() ;
    }
 
    ///////////////////////////////////////////////////////////////////
    //
    public String getOrderByListString()
    {
      String tmpStr = super.getOrderByList().toString();
      String resultStr = "";
      List<String> orderByList = new ArrayList<String>()

      def tmpStr2 = tmpStr.substring(1,tmpStr.indexOf(']'))

      ///////////////////////////////////////////////////////////////////////////////////////////////////
      // Unqualified column name used here due to HIVE-1449 (Google it)
      //
      // So the string "CUST.first_name, CUST.address" needs to become "first_name, address"
      ///////////////////////////////////////////////////////////////////////////////////////////////////

      // First find out if multiple elements in the list.  This is indicated by the existence of a ',' seperating the fields.
      def resultList = tmpStr2.tokenize(',')
      def tt = resultList.size()
      resultStr = removeQualifiedName(resultList.get(0));
      if (tt > 1) {
        // Multiple elements in the list
        for (i in 1..(resultList.size()-1)) {
          resultStr += ", ";
  
         // Now for every element in the list, find out if there is a '.' in the string.
         resultStr += removeQualifiedName(resultList.get(i));
        } // for
      } // else

      return resultStr ;
    }
  
    //////////////////////////////////////////////////////////////
    // Need to remove the fully qualified namename
    // Source : CUST.first_name
    // Result : first_name
    //
    public def removeQualifiedName(String tStr) {
      def resultList = tStr.tokenize('.')
      if (resultList.size() == 1) return tStr;
      def tResultStr = resultList.get(resultList.size()-1)
      return tResultStr
    } // removeQualifiedName()

    /**************************************************************
     * Set the ORDER BY operator to this query.
     *
     * @param sorterExp The order by operator string.
     **************************************************************/
    public void setOrderByOperator (String sortOperator)
    {
      setPropertyValue("orderByOperatorList",sortOperator);
    } // setOrderByOperatort()

    /**************************************************************
     * Set the ORDER BY expression to this query.
     *
     * @param sorterExp The order by expressions.
     **************************************************************/
    public void setOrderByList (ArrayExpression sorterExp)
    {
    } // setOrderByList()
    
  
  } // End of HiveSqlQuery class

    	        
        public class UnmappedColumnExpression extends StringExpression {
        	public UnmappedColumnExpression( String DdlDataType ) { super( "cast (null as ${DdlDataType})" ) }
        }
        
        public class SkipColumnExpression extends StringExpression {
        	public SkipColumnExpression() { super( "<SkipColumnExpression: this should never be seen.>" ) }
        }

        public class CopyColumnExpression extends StringExpression {
        	public CopyColumnExpression() { super( "<CopyColumnExpression: this should never be seen.>" ) }
        }

        //TODO: Build universal CustomAST object for passing sets of variables from KM to template
        /*
        public class SmartCustomAST extends CustomAST {
            //TODO: Why do these dynamic properties do not work?
            //public def storage = [:]
            //public propertyMissing(String name, value) { println "propertyMissing: $name $value"; storage[name] = value }
            //public def propertyMissing(String name)    { println "propertyMissing: $name"; storage[name] }
            
            public SmartCustomAST( String componentKMName, String template ) {
                super( componentKMName, template )
            }
        }
        */

        @ASTClass(descriptionKey="ASTClass.SqlCreateTableStatement")
        public class SqlCreateTableStatement extends CustomAST {
            protected Table table
            protected List<Column> columns
            protected OdiTechnology techno
            protected IOdiFlexFieldFinder ffFinder
            public    String tableType = ""
            public    String tableOptions = ""

            public SqlCreateTableStatement( String componentKMName, String template, Table pTable, OdiTechnology pTechno, IOdiFlexFieldFinder pFfFinder ) {
                super( componentKMName, template )
                techno = pTechno
                table = pTable
                ffFinder = pFfFinder
                table.initFlexFields( ffFinder )
            }
        	
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.SqlCreateTableStatement.getTable")
            public Table  getTable()          { return table }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.SqlCreateTableStatement.tableName")
            public String tableName()         { return table.getGeneratedName() }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.SqlCreateTableStatement.tableType")
            public String tableType()         { return tableType }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.SqlCreateTableStatement.tableOptions")
            public String tableOptions()      { return tableOptions }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.SqlCreateTableStatement.colListDdl")
            public String colListDdl()        { return columns.collect { col ->
            	                                                  // Is this a Virtual Column (journalization columns)?
            	                                              	  if ( col instanceof OdiDataSetRef$VirtualColumn )    
            	                                              	      return col.getName() + "\t" + col.getDdlDataType()
            	                                              	  // Has an explicit data type override been set?
            	                                              	  if ( col.mDdlDataType != null )
            	                                              	      return col.getName() + "\t" + col.mDdlDataType
            	                                              	  def refCol = col.getRefObjectOrPhysColumn()
            	                                              	  // Does this column reference a MapAttribute?
            	                                              	  if ( refCol instanceof MapAttribute || refCol instanceof MapPhysicalColumn )    
            	                                              	      //return refCol.getName() + "\t" + refCol.getDdlDataType()
            	                                              	      return col.getName() + "\t" + refCol.getDdlDataType()
            	                                              	  // If this is an OdiColumn, we may need to convert from File datatype to Hive datatype
            	                                              	  def dt = refCol.getDataType()
            	                                              	  // We convert the DT just in case the referenced object is a File DataStore
            	                                              	  def colLen = refCol.getLength()
            	                                              	  def colScale = refCol.getScale() 
            	                                              	  def dataType = dt.getConvertedDataType( techno, colLen, colScale )
            	                                              	  //return refCol.getName() + "\t" + MappingUtils.getDdlDataType( dataType, refCol.getLength(), refCol.getScale(), refCol.getName() )
                                                                  // Borrowed code from MappingUtils.getDdlDataType
                                                                  if (dataType != null && dataType.dataType != null) {
                                                                    if (dataType.expression1 != null) {
                                                                      colLen = dataType.expression1.intValue();
            	                                              }
                                                                    if (dataType.expression2 != null) {
                                                                      colScale = dataType.expression2.intValue();
                                                                    }
                                                                  }
            	                                              	  return col.getName() + "\t" + MappingUtils.getDdlDataType( dataType.dataType, colLen, colScale, col.getName() )
            	                                              }
            	                                              .join( ",\n\t" ) }
            
            public setColumns( List<IColumn> pColumns ) { columns = pColumns; columns.each { col -> col.initFlexFields( ffFinder ) } }
        }
                
        @ASTClass(descriptionKey="ASTClass.HiveCreateTableStatement")
        public class HiveCreateTableStatement extends SqlCreateTableStatement {
            //protected HiveTable table
            //protected List<Column> columns
            public Integer buckets
        	
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.external")
            public String external()          { return (table.external)?"EXTERNAL":"" }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.ifNotExists")
            public String ifNotExists()       { return (table.ifNotExists)?"IF NOT EXISTS":"" }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.colListDdl")
            public String colListDdl()        { return columns.findAll { col -> !col.isPartition() }
//            	                                              .collect { col -> col.getName() + "\t" + col.getRefObject().getDdlDataType() }
            	                                              .collect { col ->

            	                                                  // Is this a Virtual Column (journalization columns)?
            	                                              	  if ( col instanceof OdiDataSetRef$VirtualColumn )    
            	                                              	      return col.getName() + "\t" + col.getDdlDataType()
            	                                              	  // Has an explicit data type override been set?
            	                                              	  if ( col.mDdlDataType != null )
            	                                              	      return col.getName() + "\t" + col.mDdlDataType

            	                                              	  def refCol = col.getRefObjectOrPhysColumn()

            	                                              	  // Does this column reference a MapAttribute?
            	                                              	  if ( refCol instanceof MapAttribute ) {   
            	                                              	   String colName = col.getName();
            	                                              	   def dataFormat  = refCol.getBoundObject().getDataFormat();
            	                                              	       
            	                                              	      if (dataFormat != null){
            	                                              	      String parsedDataFormat = parseDataFormat(colName, dataFormat);
            	                                              	        return col.getName() + "\t" + parsedDataFormat}
            	                                              	      return col.getName() + "\t" + refCol.getDdlDataType()
            	                                              	  }

            	                                              	  // Does this column reference a MapPhysicalColumn?
            	                                              	  if ( refCol instanceof MapPhysicalColumn ) {   
            	                                              	      return col.getName() + "\t" + refCol.getDdlDataType()
            	                                              	  }

            	                                              	  // If this is an OdiColumn, we may need to convert from File datatype to Hive datatype
            	                                              	  def dataFormatOC  = refCol.getDataFormat()
            	                                              	  if (dataFormatOC != null){
            	                                              	  String parsedDataFormatOC = parseDataFormat(col.getName(), dataFormatOC);
            	                                              	    return col.getName() + "\t" + parsedDataFormatOC}
            	                                              	  def dt = refCol.getDataType()
            	                                              	  // We convert the DT just in case the referenced object is a File DataStore
            	                                              	  def hiveDataType = dt?.getConvertedDataType( techno, refCol.getLength(), refCol.getScale() )
            	                                              	  return col.getName() + "\t" + MappingUtils.getDdlDataType( hiveDataType.dataType, refCol.getLength(), refCol.getScale(), col.getName() )
            	                                              }
            	                                              .join( ",\n\t" ) }
            	                                              
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.partitionedClause")
            public String partitionedClause() { return columns.findAll { col -> col.isPartition() }
            	                                              .collect { col -> col.getName() + "\t" + col.getRefObject().getDdlDataType() }
            	                                              .join( "PARTITIONED BY (",   ",\n\t",   ")" ) }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.clusteredClause")
            public String clusteredClause()   { return columns.findAll { col -> col.isCluster() }
            	                                              .collect { col -> col.getName() }
            	                                              .join( "CLUSTERED BY (",   ", ",   ")" ) }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.sortByClause")
            public String sortByClause()      { return columns.findAll { col -> col.isSort() }
							      .sort { col -> col.getSortPosition() }
            	                                              .collect { col -> col.getName()  + " " + col.getSortDirection() }
            	                                              .join( "SORTED BY (",   ", ",   ")" ) }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.intoBucketsClause")
            public String intoBucketsClause() { return (buckets==null || buckets == 0)?"":"INTO " + buckets + " BUCKETS" }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.rowFormatClause")
            public String rowFormatClause() { return table.rowFormatClause }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveCreateTableStatement.tblPropertiesClause")
            public String tblPropertiesClause() { return table.tblPropertiesClause }
            
            public HiveCreateTableStatement( String componentKMName, String template, HiveTable pTable, OdiTechnology pHiveTechno, IOdiFlexFieldFinder ffFinder ) {
                super( componentKMName, template, pTable, pHiveTechno, ffFinder )
                buckets = table.getHiveBuckets() 
            }
            
            String parseDataFormat(String colName, String dataFormat) {
                  org.codehaus.jackson.map.ObjectMapper mapper = new org.codehaus.jackson.map.ObjectMapper();
                  JsonNode rootNode = mapper.readTree(dataFormat);
                  String complexField = processSchema(rootNode);
                  complexField = complexField.replace("integer", "int");
                  complexField = complexField.replace("bytes", "binary");
                  complexField = complexField.replace("fixed", "binary");
                  complexField = complexField.replace("long", "bigint");
                  return complexField;
        }
        
                String processSchema(JsonNode node) {
                  String complexField;
                  JsonNode typeNode = node.path("type");
                  String typeNodeText = typeNode.getTextValue();
                  if (typeNode.isContainerNode()) {
                    complexField = processSchema(typeNode);
                  } else if (typeNodeText != null && typeNodeText.equalsIgnoreCase("array")) {
                    JsonNode itemNode = node.path("items");
                    complexField = "array<" + processSchema(itemNode) + ">";
                  }else if (typeNodeText != null && typeNodeText.equalsIgnoreCase("map")) {
                    JsonNode itemNode = node.path("items");
                    complexField = "map<" + processSchema(itemNode) + ">"; 
                  }else if (typeNodeText != null && typeNodeText.equalsIgnoreCase("union")) {
                    JsonNode itemNode = node.path("items");
                    complexField = "union<" + processSchema(itemNode) + ">"; 
                  }else if (typeNodeText != null && (typeNodeText.equalsIgnoreCase("record") || typeNodeText.equalsIgnoreCase("object"))) {
                    StringBuffer cf = new StringBuffer();
                    cf.append("struct<");
                    JsonNode fieldsNode = node.path("fields");
                    Iterator<JsonNode> fields = fieldsNode.getElements();
                    while (fields.hasNext()) {
                      JsonNode fieldNode = fields.next();
                      String fieldName = fieldNode.path("name").getTextValue();
                      JsonNode fieldTypeNode = fieldNode.path("type");
                      boolean isContainerNode = fieldTypeNode.isContainerNode();
                      String type = null;
                      if (isContainerNode) {
                        cf.append(fieldName + "\\:" + processSchema(fieldTypeNode) + ","); 
                      } else {
                        type = fieldTypeNode.getTextValue();
                        cf.append(fieldName + "\\:" + type + ","); 
                      }
                    }
                    cf.deleteCharAt(cf.length()-1);
                    cf.append(">");
                    complexField = cf.toString();
                  } else if (typeNodeText != null && (typeNodeText.equalsIgnoreCase("string")|| typeNodeText.equalsIgnoreCase("fixed") || typeNodeText.equalsIgnoreCase("integer") || typeNodeText.equalsIgnoreCase("double") || typeNodeText.equalsIgnoreCase("float") || typeNodeText.equalsIgnoreCase("long") || typeNodeText.equalsIgnoreCase("number")))
                    return typeNodeText;
                  else 
                    return node.toString().replace("\"", "");

                  return complexField;
                }
        }
        
        @ASTClass(descriptionKey="ASTClass.HiveInsertStatement")
        @SubstitutionVariables([
          @SubstitutionVariable(name="INSERT",type=HiveInsertStatement.class)
        ])
        @UsesTemplate(name="HiveInsertStatement")
        public class HiveInsertStatement extends SqlInsertStatement {
            public IOdiFlexFieldFinder ffFinder
        	String partitionClause = ""
        	public Boolean overwrite = false
        	
            public HiveInsertStatement() { super() }
            public HiveInsertStatement( SqlInsertStatement otherStatement ) { super( otherStatement ); this.ffFinder = otherStatement.ffFinder;  }
            public HiveInsertStatement (Table table, List<Column> columnList, SqlQuery query, IOdiFlexFieldFinder pFfFinder ) { this( table, columnList, query, true, pFfFinder  ) }
            public HiveInsertStatement (Table table, List<Column> columnList, SqlQuery query, Boolean recalcColList, IOdiFlexFieldFinder pFfFinder ) { 
            	super( table, columnList, query )
            	ffFinder = pFfFinder
            	table.initFlexFields( ffFinder )
            	columnList.each { col -> col.initFlexFields( ffFinder ) }
            	
            	// Set a default order of KM task generated from this AST
            	setOrder( 500 )
            	

            	// Build the partition clause of the insert statement
                partitionClause = getTargetColumns().findAll { col -> col.isPartition() }
                                                    .collect { col -> col.getName() + 
                                                               // for columns with constant target values, we have to add the value:
                                                               ((col.isMapped() && !col.referencesUpstreamAttribute()) ? "=" + col.getRefObjectOrPhysColumn()?.getExpression()?.getText() : "") }
                                                    .join( "PARTITION (", ", ", ")" )                
            	if ( recalcColList ) {
            	    // Put all non-partition columns first and drop any partition columns with constant value mapping from list. 
            	    // Constant values in partition columns are covered by the partition clause
                	def targetCols = columnList
                	def selectList = query.getSelectList()
                	def aliasList  = query.getAliasList()      
                	                	
                	def reorderedSelectList    = // put all non-partition columns first. Then all other columns. Omit partition columns with constant value. Those are treated in partition clause.
                	                                 targetCols.collectWithIndex { col,idx -> (!col.isPartition())?selectList.get(idx):null }
                	                          .plus( targetCols.collectWithIndex { col,idx -> ( col.isPartition() && col.referencesUpstreamAttribute() )?selectList.get(idx):null } )
                	                          .findAll { (it!=null) }
                	def reorderedTargetColumns = // put all non-partition columns first. Then all other columns. Omit partition columns with constant value. Those are treated in partition clause.
                	                                 targetCols.findAll { col -> !col.isPartition() }
                	                          .plus( targetCols.findAll { col -> (col.isPartition() && col.referencesUpstreamAttribute()) } )
                    def reorderedAliasList = null
                	if ( aliasList != null ) {
                        reorderedAliasList     = // put all non-partition columns first. Then all other columns
                                                     targetCols.collectWithIndex { col,idx -> (!col.isPartition())?aliasList?.get(idx):null }
                                              .plus( targetCols.collectWithIndex { col,idx -> ( col.isPartition() && col.referencesUpstreamAttribute())?aliasList?.get(idx):null } )
                                              .findAll { (it!=null) }
                	}

                	def reorderedQuery = new HiveSqlQuery( query )   
                	reorderedQuery.setSelectList( reorderedSelectList )
                	reorderedQuery.setAliasList(  reorderedAliasList )
                	setSelectQuery( reorderedQuery )
                	setTargetColumns( reorderedTargetColumns )
                }

            }
            //public setText( String pText ) { text = pText }
            //public String getText() { if (text==null) { return super.getText() } else { return text } }
            //public String getText(TemplateUtils templateUtils) { return ""; if (text==null) { return super.getText(templateUtils) } else { return text } }
            //TODO: This should define the LineType. But for IKMs this does not work
            public IProcedureLine.LineType getLineType() { return IProcedureLine.LineType.EX_UNIT_MAIN; }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveInsertStatement.tableName")
            public String tableName() { return getTargetTable().getGeneratedName() }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveInsertStatement.partitionClause")
            public String partitionClause() { return partitionClause }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveInsertStatement.overwrite")
            public String overwrite() { return (overwrite)?"OVERWRITE":"INTO" }
                        
            @Override
            public void setTargetTable (Table table) {
            	if ( !(table instanceof HiveTable) ) {
            		//TODO: proper exeception handling needed
            		throw new RuntimeException( "Parameter must be instance of HiveTable!" )
                }
                super.setTargetTable( table )
            }
            
        }

        @ASTClass(descriptionKey="ASTClass.FileToHiveStatement")
        @SubstitutionVariables([
          @SubstitutionVariable(name="INSERT",type=FileToHiveStatement.class),
        ])
        @UsesTemplate(name="FileToHiveStatement")
        public class FileToHiveStatement extends HiveInsertStatement {
            private ILocation mTargetLocation
            private ILocation mSourceLocation
            private ITechnology mTargetTechnology;
            private Table sourceFile
            
            private int noPartitionColumns
            private int noUnmappedPartitionColumns
            private String codeForMixingPartitionColumnValues
            private boolean isLocalFile, isOverwrite
            
            public FileToHiveStatement( MapPhysicalNode papNode, Table pTable, List<Column> pColumnList, SqlQuery input, IOdiFlexFieldFinder ffFinder, boolean isLocalFile, boolean isOverwrite ) {
                super( pTable, pColumnList, input, false, ffFinder )
                
                this.isLocalFile = isLocalFile
                this.isOverwrite = isOverwrite
                
                mTargetLocation = papNode.getExecutionUnit().getLocation()
                mSourceLocation = papNode.getAPNodeSourceExUnit().getLocation()
                
                // Collect any source tables (there should only be a single one)
                def srcLst = input.getFromList().collect { it.getSourceTables().collect { it } }.flatten()
                if ( srcLst.size() <= 0 && srcLst.size() > 1 )
                	throw new RuntimeException( "Input query must have exactly one source file." )
                sourceFile = srcLst[0]
                
                noPartitionColumns         = getTargetColumns().findAll { col -> col.isPartition() }
                                                               .size()
                noUnmappedPartitionColumns = getTargetColumns().findAll { col -> (col.isPartition() && (!col.isMapped())) }                              
                                                               .size()
                def idx = 1
            	def txt = getTargetColumns()
            	              // get all partition columns, which are unmapped or have a constant value
                              .findAll { col -> col.isPartition() && (!col.isMapped() || !col.referencesUpstreamAttribute()) }
                              .collect { col -> 'part_col_vals.add( ' +
                                ((col.isMapped())? ("''' " + col.getRefObjectOrPhysColumn()?.getExpression()?.getText() + " '''") : 'part_col_vals_from_fileName[' + idx++ + ']') + 
                            " )\t// " + col.getRefObjectOrPhysColumn().getSQLAccessName() }
                                .join( "", "\n        ", "\n" )
                idx = 0
                txt += getTargetColumns()                
            	              // get all partition columns, which are unmapped or have a constant value
                              .findAll { col -> col.isPartition() && (!col.isMapped() || !col.referencesUpstreamAttribute()) }
                              .collect { col -> 
                              	            def delim = ""
                              	            // if the partition value comes from the file name, we have to add a string delimiter if this is a character column
                              	            if (!col.isMapped() && Column.isCharacterDataType( col.getRefObjectOrPhysColumn().getDataType() ) && (!col.isMapped()) ) 
                              	                delim = "'"
                                            return 'sql += "' + col.getName() + ' = ' + 
                                                   delim + '" + "${part_col_vals[' + idx++ + ']}" + "' + delim + 
                                                   '"\t// ' + col.getRefObjectOrPhysColumn().getSQLAccessName() }
                              .join( "        sql += 'partition ('\n        ", 
                                     "\n        sql += ', '\n        ", 
                                     "\n        sql += ')'\n" )
        	    codeForMixingPartitionColumnValues = txt                  
            }
            
            @Override
            public ILocation getTargetLocation() throws GenerationException { return mTargetLocation }
            @Override
            public ILocation getSourceLocation() throws GenerationException { return mSourceLocation }
            @Override
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.getTargetTechnology")
            public ITechnology getTargetTechnology() throws GenerationException { return mTargetTechnology }
            public void setTargetTechnology(ITechnology targetTechno) { mTargetTechnology = targetTechno }

            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.isLocalFile")
            public isLocalFile()                { return isLocalFile }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.isOverwrite")
            public isOverwrite()                { return isOverwrite }
    
			public void setOverwrite (boolean overwriteTarget) { isOverwrite = overwriteTarget } 
    	           
            //TODO: these methods are only required, because $[] expressions do not support groovy expressions
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.fileNamePattern")
            public fileNamePattern()                { return sourceFile.getName() } 
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.getSourceFileDataStore")
            public getSourceFileDataStore()         { return sourceFile.getModelTable().getBoundDataStore() } 
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.noPartitionColumns")
            public int noPartitionColumns()         { return noPartitionColumns }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.noUnmappedPartitionColumns")
            public int noUnmappedPartitionColumns() { return noUnmappedPartitionColumns }
            
            @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.FileToHiveStatement.codeForMixingPartitionColumnValues")
            public String codeForMixingPartitionColumnValues() { return codeForMixingPartitionColumnValues }  
            
            @Override
            public Map<String,Object> getTemplateSubstitutionMap() {
              Map<String,Object> result = new LinkedHashMap<String,Object>();
              result.put("INSERT", this)
              return result
            }   	
        }
        
        @ASTClass(descriptionKey="ASTClass.HiveFileInsert")
        @SubstitutionVariables([
          @SubstitutionVariable(name="HIVE_FILE_INSERT",type=HiveFileInsert.class)
        ])
        @UsesTemplate(name="LkmHiveToFile_UnloadHiveData")
        public class HiveFileInsert extends NullTarget {
          String sourceDir, fieldSeparator, rowSeparator;
          SqlQuery hiveUnloadQuery;
          
          public HiveFileInsert() {
            super() 
            skipGenerateTask();  // Tells the generator not to auto-generate a task for this AST.
          }
          
          void setSourceDir(String sourceDir) {
            this.sourceDir = sourceDir;
          }
          
          void setHiveUnloadQuery(SqlQuery query) {
            this.hiveUnloadQuery = query;
          }
          
          void setFieldSeparator(String fieldSeparator) {
            this.fieldSeparator = fieldSeparator;
          }
          
          void setRowSeparator(String rowSeparator) {
            this.rowSeparator = rowSeparator;
          }
          
          @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveFileInsert.getSourceDir")
          public String getSourceDir() {
            return this.sourceDir;
          }
          
          @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveFileInsert.getHiveUnloadQuery")
          public SqlQuery getHiveUnloadQuery() {
            return this.hiveUnloadQuery;
          }
          
          @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveFileInsert.getFieldSeparator")
          public String getFieldSeparator() {
            return this.fieldSeparator;
          }
          
          @SubstitutionAPIMethod(descKey="SubstitutionAPIMethod.HiveFileInsert.getRowSeparator")
          public String getRowSeparator() {
            return this.rowSeparator;
          }
          
          @Override
          public Map<String,Object> getTemplateSubstitutionMap() {
            Map<String,Object> result = new LinkedHashMap<String,Object>();
            result.put("HIVE_FILE_INSERT", this)
            return result
          }
          
        }
                
        @ASTClass(descriptionKey="ASTClass.HiveTable")
        public class HiveTable extends Table {
            public boolean external    = false;
            public boolean ifNotExists = false;
            public String rowFormatClause = "";
            public String tblPropertiesClause = "";
            public HiveTable (String name, String pQualifier, String customText) { HiveTable( name, pQualifier, customText, false, false ) }
            public HiveTable (String name, String pQualifier, String customText, boolean pExternal, boolean pIfNotExists) { 
            	super( name, pQualifier, customText ) 
            	external    = pExternal
            	ifNotExists = pIfNotExists
            }
            // In case of staging tables the HiveTable holds the information about clustering
            //TODO: to be implemented as part of later story
            
            public boolean isClusterColumn( Column col )   { return false }
            public boolean isPartitionColumn( Column col ) { return false }
            public boolean isSortColumn( Column col )      { return false }
            public getFlexFieldValue( String ffCode )      { return super.getFlexFieldValue( ffCode ) }
        }
        
        /*
        public class HiveColumn extends Column {
            public HiveColumn (Table table, String name, IModelObject refObject) {
                super( table, name, refObject )
            }
            static HiveColumn getColumn(Table tab, MapPhysicalColumn col) { return Column.getColumn( tab, col ) }
            public isCluster() { return false }
            public isPartition() { return false }
            public isSort() { return false }
        }
        */


        @GeneratorDelegateClass(componentType=DatastoreComponent.COMPONENT_TYPE_NAME, descriptionKey="GeneratorDelegateClass.HiveBaseKM")
        public class HiveBaseKM extends GroovyComponentGeneratorDelegateBase {
    	
    	      // class constructor; initialization is done in setBinding method
            public HiveBaseKM() {}
    	
            protected IMapComponent component;
            protected MapConnectorPoint connector;
            protected MapPhysicalNode physicalNode;
            protected ArrayList upstreamASTList;
            protected GeneratorContext generatorContext;
            protected ComponentKM componentKM;
            protected String subtype;

            // Declare KM level variables
            public IOdiFlexFieldFinder ffFinder;
            // Should the task PrepareHiveSession be included?
            protected boolean prepareHiveSession = true;
            protected ArrayList hiveProperties;
                
        /////////////////////
        // General KM methods
        /////////////////////
        
        protected mBinding = null;
        public void setBinding(Binding binding) {
            super.setBinding(binding);
            mBinding = binding;
            Map variables = binding.getVariables();
            if (variables.containsKey("component")) {
              component =  mBinding.getVariable("component");
            }
            if (variables.containsKey("connector")) {
              connector =  mBinding.getVariable("connector");
            }
            if (variables.containsKey("physicalNode")) {
              physicalNode =  mBinding.getVariable("physicalNode");
            }
            if (variables.containsKey("upstreamASTList")) {
              upstreamASTList =  mBinding.getVariable("upstreamASTList");
            }
            if (variables.containsKey("generatorContext")) {
              generatorContext =  mBinding.getVariable("generatorContext");
            }
            if (variables.containsKey("componentKM")) {
              componentKM =  mBinding.getVariable("componentKM");
            }
            if (variables.containsKey("subtype")) {
              subtype =  mBinding.getVariable("subtype");
            }             
        
        	ffFinder = (IOdiFlexFieldFinder) ((OdiAdapter)component.getRootContainer().getAdapter()).getEntityManager().getFinder(OdiFlexField.class)
        }
        
        // Publish any variables back into binding to make them available in templates and variableDefText
        protected void updateBinding() { 
        	// expose variables into Binding
        	mBinding.setVariable( "prepareHiveSession", prepareHiveSession )
        	hiveProperties = getHiveSessionProperties()
        	mBinding.setVariable( "hiveProperties", hiveProperties )
        }
        
        // Helper methods
        ITechnology getGroovyTechnology() {
          return (ITechnology) physicalNode.getAdapter().findModelObject(ITechnology.class.getName(), null, "Groovy")
        }
        
        ITechnology getHiveTechnology() {
          return (ITechnology) physicalNode.getAdapter().findModelObject(ITechnology.class.getName(), null, "Hive")
        }
        
        public String getQuotedString( String pString ) {
            def dq = ""+(char)34
            // As getQuotedJavaString does not handle any code gen expression well, we need to put some work-around:
            // if "odiRef" is found inside pString, then we just surround the pString by double-quotes.
            // Limitation: this only works as long as pString does not require any escaping. E.g. does not contain 
            // double-quotes.
            //TODO: remove work-around
            if ( pString == null )
                return SnpsStringTools.getQuotedJavaString(new StringBuffer()).toString()
            if ( pString.contains( "odiRef" ) ) 
                return dq + pString + dq
            return SnpsStringTools.getQuotedJavaString(new StringBuffer(pString)).toString()
        }
        
        // Returns the value of a KM checkbox option 
        boolean isOptionTrue( String optionName ) {
            def option = null;
            if ( option == null && physicalNode.isIKMNode() ) {
                option = physicalNode.getIKMOptionValue( optionName )
            }
            if ( option == null && physicalNode.isLKMNode() ) {
                option = physicalNode.getLKMOptionValue( optionName )
            }
            if ( option == null && physicalNode.isXKMNode() ) {
                option = physicalNode.getXKMOptionValue( optionName )
            }
            if ( option != null ) {
                return option.getOptionValue()
            } else {
                //TODO: We must not use RuntimeException. Use proper exception class instead. How to register message key?
                throw new RuntimeException( "KM option $optionName not defined" )
            }
        }     
        
        // Returns the value of a KM text option 
        String getOption( String optionName ) {
            def option = null;
            if ( option == null && physicalNode.isIKMNode() ) {
                option = physicalNode.getIKMOptionValue( optionName )
            }
            if ( option == null && physicalNode.isLKMNode() ) {
                option = physicalNode.getLKMOptionValue( optionName )
            }
            if ( option == null && physicalNode.isXKMNode() ) {
                option = physicalNode.getXKMOptionValue( optionName )
            }
            if ( option != null ) {
                return option.getOptionValueString()
            } else {
                //TODO: We must not use RuntimeException. Use proper exception class instead. How to register message key?
                throw new RuntimeException( "KM option $optionName not defined" )
            }
        }    
        
        ///////////////////////////
        // Hive specific KM methods
        ///////////////////////////
        
        List<String> getHiveSessionProperties() {
          def opt = getOption("HIVE_SESSION_PROPERTIES")
          def optList = []
          if (opt != null && opt.trim() != "") {
            optList = opt.split('\n')
          }
          return optList
        }
        
        List<Column> getColumnList( Table tgtTab ) { 
            List<Column> result = new ArrayList<Column>()
            def exprs = component.getOutputAttributeExpressions()
            for (expr in exprs) {
                def attr = (MapAttribute) expr.getOwner()
                result.add(Column.getColumn(tgtTab, attr));
            }
            return result
        }
        
        HiveTable getTargetTable() {
        	def node = physicalNode
            if ( physicalNode.isDirectLoad() ) 
                // For direct target load we have to use the parent node to construct the target table
                node = physicalNode.getParentNode()
            def tableName        = node.getBoundObjectName();  // The name of the associated datastore object
            def tableQualifier   = node.getLocation() == null ? null : node.getLocation().getName();        
            def customTgtText    = OdiRef.getOdiGeneratedAccessName("TARG_NAME", node)
            Table table          = new HiveTable( tableName, tableQualifier, customTgtText, false, true )
            def tableCmp         = node.getLogicalComponent();
            table.setModelTable( tableCmp );
            return table
        }
        
        /*
        HiveTable getErrorTable() {
        	def node = physicalNode
            def tableName        = node.getBoundObjectName();  // The name of the associated datastore object
            def tableQualifier   = node.getLocation() == null ? null : node.getLocation().getName();        
            def customTgtText    = OdiRef.getOdiGeneratedAccessName("ERR_NAME", node, "W")
            Table table          = new HiveTable( tableName, tableQualifier, customTgtText, false, true )
            def tableCmp         = node.getLogicalComponent();
            table.setModelTable( tableCmp );
            return table
        }
        */
        
        HiveTable getStagingTable() {
        	def node = physicalNode
            def tableName      = physicalNode.getStageTableName();
            def tableQualifier = OdiRef.getNodeLogicalSchemaName( physicalNode );
            def tableAlias     = physicalNode.getStageTableAlias();
            def tableCmp       = physicalNode.getLogicalComponent();
            def customTgtText
            if ( node.isLKMNode() )
                customTgtText = generatorContext.getAPNodeCollTableAccessName( physicalNode )
            else if ( node.isIKMNode() ) 
                customTgtText = OdiRef.getOdiGeneratedAccessName("INT_NAME", node, "W")
            else
                customTgtText = "<This Node type is not supported in getStagingTable()>"
            Table table        = new HiveTable( tableName, tableQualifier, customTgtText, false, true );
            table.setModelTable( null );
            return table
        }
 
        protected setDDLTargetTable(OdiDataStore sourceDataStore, OdiDataStore targetDataStore, HiveTable targetTable, String sourceSchemaName, boolean isDirectLoad) {
          boolean isExternal;
          Object hiveLocation;
          Object tblProperties;
          Object serdevalue;
          Object serdeProperties;
          Object inputfvalue;  
          Object outFvalue;
          Object fileFormat;
          Object fieldSep;  
          Object recSep;
          Object storageRowFormat;
          Object escpChar;
          Object mapKeytSep;
          Object collecSep;
          Object nullDefinedAs;
          Object storageHandler;
          Object storageHandlerProperties;

          if (targetDataStore.getHadoopStorageDescriptor() == null) 
            return;

          isExternal = !targetDataStore.getHadoopStorageDescriptor().isHiveManaged()
          hiveLocation = targetDataStore.getHadoopStorageDescriptor().getHiveLocation();
          tblProperties = targetDataStore.getHadoopStorageDescriptor().getTableProperties();
          if (targetDataStore.getHadoopStorageDescriptor().isHiveNative()) {
            serdevalue = targetDataStore.getHadoopStorageDescriptor().getSerde(); 
            serdeProperties = targetDataStore.getHadoopStorageDescriptor().getSerdeProperties();
            inputfvalue = targetDataStore.getHadoopStorageDescriptor().getInputFormat(); 
            outFvalue = targetDataStore.getHadoopStorageDescriptor().getOutputFormat();   
            fileFormat = targetDataStore.getHadoopStorageDescriptor().getFormatType();
            if (!SnpsStringTools.replaceNVL(targetDataStore.getHadoopStorageDescriptor().getFieldSeparator()).trim().equals(""))
              fieldSep = SnpsStringTools.replaceNVL(SnpsStringTools.escapeUnicode(targetDataStore.getHadoopStorageDescriptor().getFieldSeparator(), false))
            if (!SnpsStringTools.replaceNVL(targetDataStore.getHadoopStorageDescriptor().getRowSeparator()).trim().equals(""))
              recSep = SnpsStringTools.replaceNVL(SnpsStringTools.escapeUnicode(SnpsStringTools.hexStrToStr(targetDataStore.getHadoopStorageDescriptor().getRowSeparator()), true))
            storageRowFormat = targetDataStore.getHadoopStorageDescriptor().getHiveRowFormat();
            escpChar = targetDataStore.getHadoopStorageDescriptor().getEscapeCharacter();
            mapKeytSep = targetDataStore.getHadoopStorageDescriptor().getMapKeysSeparator();
            collecSep = targetDataStore.getHadoopStorageDescriptor().getCollectionSeparator();
            nullDefinedAs = targetDataStore.getHadoopStorageDescriptor().getNullDefinedAs();
          } else {
            storageHandler = targetDataStore.getHadoopStorageDescriptor().getStorageHandler();
            storageHandlerProperties = targetDataStore.getHadoopStorageDescriptor().getStorageProperties();
          }

          String rowFormatClause = ""
          String tblPropertiesClause = ""
          String serde = ""
          String inputformat = ""
          String outputformat = ""
          String fileFormatName = ""
          String hiveloc = ""

          if ( serdevalue != null) {
            serde = serdevalue.toString();
          }
          if ( inputfvalue != null) {
            inputformat = inputfvalue.toString();
          }
          if (outFvalue != null) {
            outputformat = outFvalue.toString();
          }
          if (fileFormat != null) {
            fileFormatName = fileFormat.getName().toString();
          }
          if ( hiveLocation != null) {
            hiveloc = hiveLocation.toString();
          }

          String avroSchema = null;
          if (targetDataStore.getHadoopStorageDescriptor().isHiveNative()) {
            if (storageRowFormat != null && storageRowFormat == oracle.odi.domain.model.OdiDataStore.HadoopStorageDescriptor.HiveRowFormat.DELIMITED) {
              rowFormatClause  += """ROW FORMAT DELIMITED\n"""
              rowFormatClause  +=  """FIELDS TERMINATED BY '""" + fieldSep.toString() + """' \n""" 
              if ( escpChar != null) {rowFormatClause  += """ESCAPED BY '""" + escpChar.toString() + """' \n"""}
              if (collecSep != null) {rowFormatClause  += """COLLECTION ITEMS TERMINATED BY '""" + collecSep.toString() + """' \n"""}
              if (  mapKeytSep != null) {rowFormatClause  +=  """MAP KEYS TERMINATED BY '""" + mapKeytSep.toString() + """' \n"""}
              if ( recSep != null) {rowFormatClause  +=  """LINES TERMINATED BY '""" +  recSep.toString() + """' \n"""} 
              if ( nullDefinedAs != null) {rowFormatClause  += """NULL DEFINED AS '""" + nullDefinedAs.toString() + """' \n"""} 
              rowFormatClause  += """STORED AS """ 
            } else if (storageRowFormat != null && storageRowFormat == oracle.odi.domain.model.OdiDataStore.HadoopStorageDescriptor.HiveRowFormat.SERDE) {  
              rowFormatClause  = """ROW FORMAT SERDE '""" + serde + """' \n""";
              if (serdeProperties != null) {
                String serdePropertiesValue = checkForQuotes(serdeProperties.toString());
                rowFormatClause += """WITH SERDEPROPERTIES (""" + serdePropertiesValue + """) \n"""; 
              }
              rowFormatClause += """STORED AS """; 
            } else if (storageRowFormat != null && storageRowFormat == oracle.odi.domain.model.OdiDataStore.HadoopStorageDescriptor.HiveRowFormat.BUILT_IN) {  
              if("JSON".equalsIgnoreCase(fileFormatName)){
                rowFormatClause  = """ROW FORMAT SERDE '""" + "org.apache.hive.hcatalog.data.JsonSerDe"  + """' \n""" + """STORED AS """
              }else {
                rowFormatClause  = """STORED AS """
              }                 
            }
            if (inputformat.trim().length() > 0 && outputformat.trim().length() > 0 && "INPUTFORMAT".equalsIgnoreCase(fileFormatName)) {
              rowFormatClause += """\nINPUTFORMAT '""" + inputformat + """' \n""" + """OUTPUTFORMAT '""" + outputformat + """' \n""" 
              if ( isExternal )
                rowFormatClause += """\nLOCATION '""" + hiveloc + """' \n"""
              if (("avro".contains(inputformat) || "avro".contains(outputformat)) && isDirectLoad)
                avroSchema = getAvroSchema(sourceDataStore, sourceSchemaName);
            } else if (fileFormatName.trim().length() > 0 && !"JSON".equalsIgnoreCase(fileFormatName)) {
              rowFormatClause += fileFormatName  + """ \n"""
              if ( isExternal )
                rowFormatClause += """\nLOCATION '""" + hiveloc + """' \n"""
              if ("AVRO".equalsIgnoreCase(fileFormatName) && isDirectLoad)
                avroSchema = getAvroSchema(sourceDataStore, sourceSchemaName);
            } else {
              if (storageRowFormat != null)
                rowFormatClause += """TEXTFILE\n"""
              if ( isExternal )
                rowFormatClause += """\nLOCATION '""" + hiveloc + """' \n"""
            }
          } else {
            rowFormatClause += """STORED BY '""" + storageHandler.toString() + """'\n""";
            if (storageHandlerProperties != null) {
              String storageHandlerPropertiesValue = checkForQuotes(storageHandlerProperties.toString());
              rowFormatClause += """WITH SERDEPROPERTIES (""" + storageHandlerPropertiesValue + """)\n""";
            }
            if ( isExternal )
              rowFormatClause += """\nLOCATION '""" + hiveloc + """' \n"""
          }

          if (tblProperties != null || avroSchema != null) {
            tblPropertiesClause += """TBLPROPERTIES (""";
            if (avroSchema != null)
              tblPropertiesClause += """'avro.schema.url'='""" + avroSchema + """'"""; 
            if (tblProperties != null) {
              def tblPropertiesVal = checkForQuotes(tblProperties.toString());
              if (avroSchema != null)
                tblPropertiesClause += """,""" + tblPropertiesVal + """)\n""";
              else
                tblPropertiesClause += tblPropertiesVal + """)\n""";
            } else
              tblPropertiesClause += """)\n""";
          }


          targetTable.external = isExternal
          targetTable.rowFormatClause = rowFormatClause
          targetTable.tblPropertiesClause = tblPropertiesClause
        }

        protected String getAvroSchema(OdiDataStore sourceDataStore, String sourceSchemaName) {
          if (sourceDataStore.getHadoopStorageDescriptor() != null) {
            String storageFormat = sourceDataStore.getHadoopStorageDescriptor().getFormatType().getName();
            if ("AVRO".equals(storageFormat)) {
              String schemaFile = sourceDataStore.getHadoopStorageDescriptor().getSchemaFile();
              if(schemaFile != null)   
                return sourceSchemaName + (schemaFile.substring(schemaFile.lastIndexOf('/')));
            }
          }
          return null;
        }

        protected String checkForQuotes(String inProperties) {
          // Protect against null and empty Strings
          if (inProperties == null || inProperties.equals(""))  {
	    return inProperties;
          }
          inProperties = inProperties.substring(1, inProperties.length()-1);
          StringBuffer outPropertiesBuf = new StringBuffer();
          int pos = inProperties.indexOf("\",\"");
          String nextToken;
          while (pos >= 0 || inProperties.length() > 0) {
            if (pos >= 0)
              nextToken = inProperties.substring(1, pos);
            else 
              nextToken = inProperties.substring(1, inProperties.length()-1);
            nextToken = nextToken.replaceAll("\"=\"", "=");
            if (nextToken.indexOf("\"") >= 0) {
              nextToken = nextToken.replaceAll("=", "'='");
              nextToken = "'" + nextToken + "'";
            } else {
              nextToken = nextToken.replaceAll("=", "\"=\"");
              nextToken = "\"" + nextToken + "\"";
            }
            outPropertiesBuf.append(nextToken + ",");
            if (pos >= 0) {
              inProperties = inProperties.substring(pos+2);
              pos = inProperties.indexOf("\",\"");
            } else {
              inProperties = "";
              pos = -1;
            }
          }

          String outProperties = outPropertiesBuf.toString();
          return outProperties.substring(0, outProperties.length()-1);
        }

        // Derived from SQLDatastoreCompKM:
        /**
         * Gets the source and target expressions needed to create an insert statement. This is a consolidated method to get
         * both lists by iterating one loop, keeping the two lists in sync. In certain cases where RM input signatures are not
         * connected, inputs can be skipped, and processing both lists in one loop allows corresponding target columns to be
         * skipped as well.
         * @param tgtTab target table.
         * @param inputQuery input query used in creating the selected list
         * @param selectList list of columns for the SELECT part of the sql statement
         * @param tgtCols list of columns for the INSERT part of the statement
         */
        protected SqlQuery getSourceAndTargetExpressionsAndQuery(Table tgtTab, SqlQuery inputQuery, List<ISelectItem> pSelectList, List<Column> pTgtCols, Closure forEachAttribute )  
        throws AdapterException, MappingException, GenerationException {
            def tgtCols    = new ArrayList<Column>();
            def selectList = new ArrayList<ISelectItem>();    	
            List<MapPhysicalExpression> exprs = physicalNode.getSortedTargetPhysicalExpressions();

            def newQuery = inputQuery.getTargetQuery( exprs );  // normally, just returns the input query itself if an inline view is not needed
	
            Set<MapAttribute> processed = new HashSet<MapAttribute>();
            for (MapPhysicalExpression expr : exprs) {
                MapAttribute attr = (MapAttribute) expr.getRefLogicalExpression().getOwner();
                if (processed.contains(attr))
                    continue ;
                processed.add(attr);
                try {
                    // An unconnected input signature will prevent an ArrayExpression from being created and an exception will
                    // be thrown (to be caught and ignored below. That is a valid reason to simply skip that attribute instead of flagging a serious error.
                    ArrayExpression arrayExpr = new ArrayExpression(physicalNode, expr, newQuery);
                    // Remember the target column name to allow linking the expression further down
                    arrayExpr.setTargetColumnName( attr.getName() )
                    selectList.add(arrayExpr);
                    //tgtCols.add(Column.getColumn(tgtTab, attr));
                }
                catch (RMUnconnectedInputException me) {
                    // An exception for an unconnected input signature is possible. Just continue, effectively skipping the attribute.
                }
            }
            newQuery.setSelectList( selectList )
            
            // Now we may have to extend the list of columns to cover any unmapped columns...
            def attributes 
            if ( physicalNode.isIKMNode() ) {
                // for IKMs we have to iterate over all node attributes
                attributes = component.getAttributes()
            } else 
            // for direct LKMs we have to iterate over all *target* node attributes
            if ( physicalNode.isDirectLoad() ) {
                attributes = physicalNode.getParentNode().getNodeAttributes()
            // for standard LKMs we have to iterate over all physical Columns
            } else {  
                attributes = physicalNode.getPhysicalColumns()
            }

            def extendedAliasList = []
                tgtCols           = []
            def extendedSelectList = attributes.collect { attr -> def expr = forEachAttribute( attr )       	  
                                                                  // Skip this column? 
                                                                  if ( expr instanceof SkipColumnExpression ) 
                                                                      return null
                                                                  // Copy existing selItem?                                                                 
                                                                  if ( expr instanceof CopyColumnExpression )
                                                                      expr = selectList.find { selItem -> (selItem.getTargetColumnName().equals( attr.getName() )) }
                                                                  // Append to target colList
                                                                  def col = Column.getColumn(tgtTab, attr)
                                                                  def alias
                                                                  // In case this is a C$ column...
                                                                  if ( attr instanceof MapPhysicalColumn ) {
                                                                      // we have to explicitly set a reference to this MapPhysicalColumn
                                                                      // This is now done automatically in Groovy-overridden getColumn method
                                                                      //col.setPhysColumn( attr )
                                                                      alias = attr.getName()
                                                                  } else {
                                                                  	 alias = attr.getSQLAliasName()
                                                                  }
                                                                  tgtCols.add( col )
                                                                  // Append col alias
                                                                  extendedAliasList.add( new StringExpression( alias ) )
                                                                  return expr }
                                               // Remove any skipped columns
            	                               .findAll { it!=null }
            newQuery.setSelectList( extendedSelectList )	
            newQuery.setAliasList( extendedAliasList )            
            
            // Add selectItems and target columns to return parameters    
            pSelectList.addAll( extendedSelectList )
            pTgtCols.addAll( tgtCols )
            //TODO:                
            // Put all non-partition columns first and drop any partition columns with constant value mapping from list. 
            // Constant values in partition columns are covered by the partition clause
                      	
            return newQuery
        }
                                
        /**
         * Create properties that reflect the component KM option values for the physical target node
         * @param insertStmt The insertStatement to copy all KM options to
         */
        void initTargetAST ( SqlInsertStatement insertStmt ) {
            // 
            def optValues = physicalNode.getOptionValues("TARGET")
            for (optValue in optValues) {
                insertStmt.setPropertyValue(optValue.getName(), optValue.getValue());
            }
        } 


        /**
         * getHiveHBaseSerDeTable returns a HiveCreateTable AST representing an Hive-HBase-SerDe table.
         * @hiveTable Hive table definition (used for table name)
         * @hBaseTableName name of HBase table, which will back the Hive-HBase-SerDe table
         * @physicalNode physicalNode to retrieve physical columns from.
         */
        protected AbstractSyntaxTree getHiveHBaseSerDeTable( HiveTable hiveTable, String hBaseTableName, MapPhysicalNode physicalNode ) {
            StringBuffer hBaseColumnsMapping = new StringBuffer()
            Closure appendCol = { String colName, OdiColumn hBaseCol ->
                                                                     
                                  // Append column to hBase column mapping
                                  hBaseCol.initFlexFields( ffFinder )
                                  if (colName.equalsIgnoreCase("key")) {
                                      hBaseColumnsMapping.append( ":key" )
                                  } else {
                                      hBaseColumnsMapping.append( hBaseCol.getName() )
                                      // Append column storage type                                                                                   
                                      if ( hBaseCol.getFlexFieldValue("HBASE_STORAGE_TYPE").equals("STRING") ) {
                                          hBaseColumnsMapping.append( "#s" )
                                      } else {
                                          hBaseColumnsMapping.append( "#b" )
                                      }
                                  }                                                                              
                                  // Append comma
                                  hBaseColumnsMapping.append( "," )
                                  def c = new Column( hiveTable, colName, hBaseCol )
                                  return  c
                                 }
            def colList
            if ( physicalNode.isDirectLoad() ) {
                colList = physicalNode.getParentNode().getSortedAllPhysicalExpressions().collect { expr -> 
                                  def colName = expr.getRefObject().getSQLAliasName( getHiveTechnology() )
                                  def hBaseCol = expr.getRefObject().getBoundObject() 
                                  return appendCol( colName, hBaseCol )                                          
                                  }
            } else {
                colList = physicalNode.getPhysicalColumns().collect { col -> 
                                  def colName = col.getName()                                                                              
                                  def hBaseCol = col.getRefObject().getBoundObject()
                                  return appendCol( colName, hBaseCol )                                          
                                  }                             
            }
              
            if ( hBaseColumnsMapping.length() > 0 )
                hBaseColumnsMapping.setLength( hBaseColumnsMapping.length()-1 )
                                                                            
            def rowFormat = "STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'\n" +
                            "WITH SERDEPROPERTIES (\n" +
                            """    "hbase.columns.mapping" = "${hBaseColumnsMapping}"\n""" +
                            """)\n""" +
                            """TBLPROPERTIES ("hbase.table.name" = "${hBaseTableName}")"""
            hiveTable.rowFormatClause = rowFormat
            hiveTable.external = true
            hiveTable.ifNotExists = false
             
            def stgTableAST = new HiveCreateTableStatement( "abc", "def", hiveTable, getHiveTechnology(), ffFinder )
            stgTableAST.setColumns( colList )                            
            return stgTableAST
        }
       
        }
    ]]></Field>
	<Field name="ExpectedAstClass" type="java.lang.String">null</Field>
	<Field name="ExtVersion" type="java.lang.String">null</Field>
	<Field name="FirstDate" type="java.sql.Timestamp"><![CDATA[2019-05-17 22:11:11.0]]></Field>
	<Field name="FirstUser" type="java.lang.String"><![CDATA[SUNOPSIS_INSTALL]]></Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[423bbc73-54a3-4f85-9553-62ac65a21751]]></Field>
	<Field name="IndChange" type="java.lang.String"><![CDATA[U]]></Field>
	<Field name="IndExcludeExUnitBegin" type="java.lang.String">null</Field>
	<Field name="IndExcludeExUnitEnd" type="java.lang.String">null</Field>
	<Field name="IndExcludeExUnitMain" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapBegin" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapCleanup" type="java.lang.String">null</Field>
	<Field name="IndExcludeMapEnd" type="java.lang.String">null</Field>
	<Field name="IndGenerateLoad" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IndIsHidden" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IndJrnMethod" type="java.lang.String">null</Field>
	<Field name="IndSuppSetBased" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IntgType" type="java.lang.String"><![CDATA[SELECT]]></Field>
	<Field name="IntVersion" type="com.sunopsis.sql.DbInt"><![CDATA[186]]></Field>
	<Field name="IsConcurrent" type="java.lang.String">null</Field>
	<Field name="IsSeeded" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="IBaseCompKm" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IFolder" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IProject" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScBaseTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScOrigTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IScTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ITrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
	<Field name="ITxtDelTxt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ITxtTrtTxt" type="com.sunopsis.sql.DbInt"><![CDATA[162]]></Field>
	<Field name="KimMultiDserver" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KmDefault" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KmLang" type="java.lang.String"><![CDATA[SQL]]></Field>
	<Field name="KmSrcLang" type="java.lang.String">null</Field>
	<Field name="KmSrcTechno" type="java.lang.String">null</Field>
	<Field name="KmTechno" type="java.lang.String"><![CDATA[HIVE]]></Field>
	<Field name="KmVersion" type="java.lang.String">null</Field>
	<Field name="LastDate" type="java.sql.Timestamp"><![CDATA[2019-10-30 21:56:02.0]]></Field>
	<Field name="LastUser" type="java.lang.String"><![CDATA[SUPERVISOR]]></Field>
	<Field name="LkmType" type="java.lang.String"><![CDATA[N]]></Field>
	<Field name="LChecksum" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="LCode" type="java.lang.String">null</Field>
	<Field name="OggJkm" type="java.lang.String">null</Field>
	<Field name="OrdFolder" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ProcType" type="java.lang.String"><![CDATA[N]]></Field>
	<Field name="ProdAstType" type="java.lang.String"><![CDATA[oracle.odi.mapping.generation.SqlQuery]]></Field>
	<Field name="RepGuid" type="java.lang.String">null</Field>
	<Field name="RepVersion" type="java.lang.String">null</Field>
	<Field name="ScriptPath" type="java.lang.String">null</Field>
	<Field name="ScOrigTrtTag" type="java.lang.String">null</Field>
	<Field name="Subtype" type="java.lang.String"><![CDATA[SELECT*]]></Field>
	<Field name="TrtName" type="java.lang.String"><![CDATA[HiveBaseKM]]></Field>
	<Field name="TrtType" type="java.lang.String"><![CDATA[CK]]></Field>
	<Field name="VariableDefs" type="java.lang.String">null</Field>
	<Field name="VLastDate" type="java.sql.Timestamp">null</Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[97c0eb01-1361-42ca-948b-1f98db1060b3]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[37]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[870bd800-fc36-4b47-87e8-7c90cbb8e00e]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[30]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[9ba5f082-8b53-40fc-a824-0f47f46d73d6]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[39c71597-fc4f-4399-94dd-e762562bc4f1]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[200]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[abfc77ae-f313-4198-8ef0-7892b1491df3]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[201]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[5b6b3327-1c26-4920-8bce-b33b14495830]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[10]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpKmTemplateJoin">
		<Field name="GlobalId" type="java.lang.String"><![CDATA[def9d821-0944-4356-8b3a-9a4d67528e43]]></Field>
	<Field name="ISnpKmTemplate" type="com.sunopsis.sql.DbInt"><![CDATA[9]]></Field>
	<Field name="ISnpTrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpTxtHeader">
		<Field name="Enc" type="java.lang.String">null</Field>
	<Field name="EncKey" type="java.lang.String">null</Field>
 <Field name="EncKeyVect" type="java.lang.String">null</Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[a9f0e6e0-b856-4b07-bf09-a5e7c66cc6ed]]></Field>
	<Field name="ITxt" type="com.sunopsis.sql.DbInt"><![CDATA[162]]></Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[107]]></Field>
	<Field name="SqlIndGrp" type="java.lang.String"><![CDATA[0]]></Field>
 <Field name="Txt" type="java.lang.String"><![CDATA[Internal - Base KM for all Hive KMs]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpOrigTxt">
		<Field name="GlobalId" type="java.lang.String">null</Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[107]]></Field>
	<Field name="OrigineName" type="java.lang.String"><![CDATA[Edit Command]]></Field>
	<Field name="SnpsCol" type="java.lang.String"><![CDATA[I_TXT_TRT_TXT]]></Field>
	<Field name="SnpsTable" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpUserExit">
		<Field name="ExprITxt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ExtVersion" type="java.lang.String">null</Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[91550d5f-6262-4f4a-85ab-af8e334cd8c1]]></Field>
	<Field name="IndChange" type="java.lang.String"><![CDATA[I]]></Field>
	<Field name="ITrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
	<Field name="ITxtUeHelp" type="com.sunopsis.sql.DbInt"><![CDATA[164]]></Field>
	<Field name="ITxtValue" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IUserExit" type="com.sunopsis.sql.DbInt"><![CDATA[33]]></Field>
	<Field name="IUserExitGroup" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="Position" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="ShortValue" type="java.lang.String">null</Field>
	<Field name="Subtypes" type="java.lang.String">null</Field>
	<Field name="UeName" type="java.lang.String"><![CDATA[HIVE_SESSION_PROPERTIES]]></Field>
	<Field name="UeSdesc" type="java.lang.String"><![CDATA[Additional Hive session properties]]></Field>
	<Field name="UeType" type="java.lang.String"><![CDATA[T]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpTxtHeader">
		<Field name="Enc" type="java.lang.String">null</Field>
	<Field name="EncKey" type="java.lang.String">null</Field>
 <Field name="EncKeyVect" type="java.lang.String">null</Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[b72e3dd9-c5f1-489c-ab74-46610542c7de]]></Field>
	<Field name="ITxt" type="com.sunopsis.sql.DbInt"><![CDATA[164]]></Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[111]]></Field>
	<Field name="SqlIndGrp" type="java.lang.String"><![CDATA[0]]></Field>
 <Field name="Txt" type="java.lang.String"><![CDATA[
    
Set properties you wish to be included in ODI Hive session for this KM.
Use the following format (one property per line) :
<property_name>=<property_value>
Example:
mapreduce.job.priority=HIGH
mapreduce.job.queuename=MY_QUEUE
mapreduce.job.max.split.locations=5
mapreduce.job.name=MY_JOB
    
    ]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpOrigTxt">
		<Field name="GlobalId" type="java.lang.String">null</Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[111]]></Field>
	<Field name="OrigineName" type="java.lang.String"><![CDATA[User Exit help]]></Field>
	<Field name="SnpsCol" type="java.lang.String"><![CDATA[I_TXT_UE_HELP]]></Field>
	<Field name="SnpsTable" type="java.lang.String"><![CDATA[SNP_USER_EXIT]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpLineTrt">
		<Field name="AlwaysExe" type="java.lang.String"><![CDATA[1]]></Field>
	<Field name="BrpId" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ColConnectId" type="java.lang.String">null</Field>
	<Field name="ColContextCode" type="java.lang.String">null</Field>
	<Field name="ColIndCommit" type="java.lang.String">null</Field>
	<Field name="ColIsolLevel" type="java.lang.String">null</Field>
	<Field name="ColITxt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ColLanguage" type="java.lang.String">null</Field>
	<Field name="ColLschemaName" type="java.lang.String">null</Field>
	<Field name="ColTechno" type="java.lang.String">null</Field>
	<Field name="DefConnectId" type="java.lang.String">null</Field>
	<Field name="DefContextCode" type="java.lang.String">null</Field>
	<Field name="DefIndCommit" type="java.lang.String">null</Field>
	<Field name="DefIsolLevel" type="java.lang.String"><![CDATA[D]]></Field>
	<Field name="DefITxt" type="com.sunopsis.sql.DbInt"><![CDATA[163]]></Field>
	<Field name="DefLanguage" type="java.lang.String"><![CDATA[GROOVY]]></Field>
	<Field name="DefLschemaName" type="java.lang.String">null</Field>
	<Field name="DefTechno" type="java.lang.String"><![CDATA[GROOVY]]></Field>
	<Field name="ExeChannel" type="java.lang.String">null</Field>
	<Field name="FirstDate" type="java.sql.Timestamp">null</Field>
	<Field name="FirstUser" type="java.lang.String">null</Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[90df928c-2caa-4fe3-a593-5a43ae50b26c]]></Field>
	<Field name="IndErr" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IndLogFinalCmd" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="IndLogNb" type="java.lang.String">null</Field>
	<Field name="ILineTrt" type="com.sunopsis.sql.DbInt"><![CDATA[63]]></Field>
	<Field name="IRefLineTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="IRefTrt" type="com.sunopsis.sql.DbInt"><![CDATA[null]]></Field>
	<Field name="ITrt" type="com.sunopsis.sql.DbInt"><![CDATA[53]]></Field>
	<Field name="KcmAk" type="java.lang.String">null</Field>
	<Field name="KcmCond" type="java.lang.String">null</Field>
	<Field name="KcmErrDel" type="java.lang.String">null</Field>
	<Field name="KcmJoin" type="java.lang.String">null</Field>
	<Field name="KcmNull" type="java.lang.String">null</Field>
	<Field name="KcmPk" type="java.lang.String">null</Field>
	<Field name="KimDrvdSel" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KimIdx" type="java.lang.String">null</Field>
	<Field name="KimJrn" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KimJrnPop" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KjmCreate" type="java.lang.String">null</Field>
	<Field name="KjmDrop" type="java.lang.String">null</Field>
	<Field name="KjmExtend" type="java.lang.String">null</Field>
	<Field name="KjmInstall" type="java.lang.String">null</Field>
	<Field name="KjmLock" type="java.lang.String">null</Field>
	<Field name="KjmPurge" type="java.lang.String">null</Field>
	<Field name="KjmSetInstall" type="java.lang.String">null</Field>
	<Field name="KjmSetUninstall" type="java.lang.String">null</Field>
	<Field name="KjmSubscribe" type="java.lang.String">null</Field>
	<Field name="KjmTableOrder" type="java.lang.String">null</Field>
	<Field name="KjmUninstall" type="java.lang.String">null</Field>
	<Field name="KjmUnlock" type="java.lang.String">null</Field>
	<Field name="KjmUnsubscribe" type="java.lang.String">null</Field>
	<Field name="KlmAfterInt" type="java.lang.String">null</Field>
	<Field name="KlmIdx" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="KlmJrn" type="java.lang.String"><![CDATA[0]]></Field>
	<Field name="LastDate" type="java.sql.Timestamp"><![CDATA[2019-10-30 21:52:34.0]]></Field>
	<Field name="LastUser" type="java.lang.String"><![CDATA[SUPERVISOR]]></Field>
	<Field name="LineType" type="java.lang.String"><![CDATA[X]]></Field>
	<Field name="LogLevDet" type="java.lang.String"><![CDATA[3]]></Field>
	<Field name="MapTaskType" type="java.lang.String"><![CDATA[MB]]></Field>
	<Field name="OrdTrt" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="SqlName" type="java.lang.String"><![CDATA[Prepare Hive Session]]></Field>
	<Field name="SupportedSubtypes" type="java.lang.String"><![CDATA[INSERT]]></Field>
	<Field name="VariableDefs" type="java.lang.String"><![CDATA[
        skipTaskGeneration = !prepareHiveSession
        addProperties = false 
        hiveSessionProperties = hiveProperties
        if (hiveProperties.size() > 0) {addProperties = true}
        ]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpTxtHeader">
		<Field name="Enc" type="java.lang.String">null</Field>
	<Field name="EncKey" type="java.lang.String">null</Field>
 <Field name="EncKeyVect" type="java.lang.String">null</Field>
	<Field name="GlobalId" type="java.lang.String"><![CDATA[74365020-2618-4cf6-82bc-25ff72887dcc]]></Field>
	<Field name="ITxt" type="com.sunopsis.sql.DbInt"><![CDATA[163]]></Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[102]]></Field>
	<Field name="SqlIndGrp" type="java.lang.String"><![CDATA[2]]></Field>
 <Field name="Txt" type="java.lang.String"><![CDATA[{# INCLUDE = 'PrepareHiveSessionTarget' #}]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpOrigTxt">
		<Field name="GlobalId" type="java.lang.String">null</Field>
	<Field name="ITxtOrig" type="com.sunopsis.sql.DbInt"><![CDATA[102]]></Field>
	<Field name="OrigineName" type="java.lang.String"><![CDATA[Target Technology]]></Field>
	<Field name="SnpsCol" type="java.lang.String"><![CDATA[DEF_I_TXT]]></Field>
	<Field name="SnpsTable" type="java.lang.String"><![CDATA[SNP_LINE_TRT]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TXTHEADER.162]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[a9f0e6e0-b856-4b07-bf09-a5e7c66cc6ed]]></Field>
 <Field name="RefObjFQName" type="java.lang.String">null</Field>
 <Field name="RefObjFQType" type="java.lang.String">null</Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String">null</Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TRT.53]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[423bbc73-54a3-4f85-9553-62ac65a21751]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[HiveBaseKM]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_TRT]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[10]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.37]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46d8e4-2d00-11e6-93c8-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[TableFunctionReference]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[22]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.30]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[c4633cfe-e55c-4fc2-8556-c2a8de93b01c]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SourceMapAttribute]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[18]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.1]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46d13c-2d00-11e6-93ac-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[SqlQuery]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[8]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.200]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46f0ae-2d00-11e6-9443-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[PrepareHiveSessionTarget]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[24]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.201]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46f0d6-2d00-11e6-9444-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[PrepareHiveSession]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[18]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.10]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46f3a6-2d00-11e6-9457-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[Table]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[5]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE.9]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[fa46f3ce-2d00-11e6-9458-00163e1ffd72]]></Field>
 <Field name="RefObjFQName" type="java.lang.String"><![CDATA[JoinTable]]></Field>
 <Field name="RefObjFQType" type="java.lang.String"><![CDATA[SNP_KMTEMPLATE]]></Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String"><![CDATA[9]]></Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TXTHEADER.164]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[b72e3dd9-c5f1-489c-ab74-46610542c7de]]></Field>
 <Field name="RefObjFQName" type="java.lang.String">null</Field>
 <Field name="RefObjFQType" type="java.lang.String">null</Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String">null</Field>
</Object>
<Object class="com.sunopsis.dwg.dbobj.SnpFKXRef">
		<Field name="RefKey" type="java.lang.String"><![CDATA[SNP_TXTHEADER.163]]></Field>
	<Field name="RefObjGlobalId" type="java.lang.String"><![CDATA[74365020-2618-4cf6-82bc-25ff72887dcc]]></Field>
 <Field name="RefObjFQName" type="java.lang.String">null</Field>
 <Field name="RefObjFQType" type="java.lang.String">null</Field>
 <Field name="RefObjFQNameLengths" type="java.lang.String">null</Field>
</Object>
<Object class="com.sunopsis.dwg.DwgExportSummary">
		<Field name="ExpTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="InstObjNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="JoinNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyColNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="KeyNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="LinkDiagNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MorigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="MtxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OrigTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[3]]></Field>
	<Field name="OtherObjectsNb" type="com.sunopsis.sql.DbInt"><![CDATA[10]]></Field>
	<Field name="PlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="StepNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="TxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[3]]></Field>
	<Field name="UeOrigNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="UeUsedNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="VarPlanAgentNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="ScenTxtNb" type="com.sunopsis.sql.DbInt"><![CDATA[0]]></Field>
	<Field name="OdiVersion" type="java.lang.String"><![CDATA[12.2.1]]></Field>
	<Field name="OriginRepositoryID" type="com.sunopsis.sql.DbInt"><![CDATA[1]]></Field>
	<Field name="RepositoryVersion" type="java.lang.String"><![CDATA[05.02.02.08]]></Field>
</Object>
</SunopsisExport>
